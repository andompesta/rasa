{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RASA](http://i.imgur.com/aImJD4o.png)\n",
    "\n",
    "\n",
    "## Instructions\n",
    "The point of this exercise is to give you the chance to show us what you know, can do, and how well you communicate what you find out. \n",
    "\n",
    "For this exercise to be useful we of course can't have solutions on the internet.\n",
    "This should go without saying but please don't distribute these questions in any form.\n",
    "\n",
    "When you feel you're ready please email your solution, with cell output, to the email address that sent this.\n",
    "If you like you can also include pdf/html exports in addition to the `.ipynb` file itself. **In your email you must include a statement that this is your original work and was completed only by you.**\n",
    "\n",
    "The exercise uses python, and the `numpy` and `matplotlib` libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: warm-up exercise.\n",
    "\n",
    "The function below draws a sample from an unnormalised discrete distribution.\n",
    "1. Explain how it works\n",
    "2. Write a vectorised version, which takes the number of samples to generate as an extra parameter, and returns a numpy array of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_discrete(b):\n",
    "    r = np.sum(b)*np.random.random()\n",
    "    a = b[0].copy()\n",
    "    i = 0\n",
    "    while a < r:\n",
    "        i += 1\n",
    "        a += b[i]\n",
    "    return i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def sample_vectoraized_discrete(b, n_samples):\n",
    "    # sample intervals\n",
    "    r = np.sum(b)*np.random.random([n_samples, 1])\n",
    "    # compute cumulative probability to compare with the sampled intervals\n",
    "    a = np.cumsum(b)\n",
    "    # return the index of the biggest cumulative value smaller than the sampled value.\n",
    "    # Note that this operation is done sample wise.\n",
    "    return np.sum(a < r, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of how it works, with probabilities `b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "b = np.array([0.1,0.2,0.3,0.4,0.3,0.5,0.2,0.03])\n",
    "n_samples = 10000\n",
    "bins = np.arange(b.size) - 0.5\n",
    "\n",
    "np.random.seed(0)\n",
    "samples = np.array([sample_discrete(b) for _ in range(n_samples)])\n",
    "\n",
    "# reset the seed to obtain equal sample for comparison\n",
    "np.random.seed(0)\n",
    "vectorized_samples = sample_vectoraized_discrete(b, n_samples)\n",
    "\n",
    "# make sure that code is correct\n",
    "assert np.all(samples == vectorized_samples)\n",
    "\n",
    "plt.hist(samples,bins, density=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Explanation 1\n",
    "\n",
    "Drawing samples from any probability distribution $P(X)$ is an important task since each samples can be used to\n",
    "approximate expectations of functions depending on $P(X)$.\n",
    "\n",
    "For any discrete distribution, given a probability mass function $p(x) = P(X=x)$; it's cumulative distribution function is:\n",
    "\n",
    "$$F(x_j) = \\sum_{i \\leq j} p(x_i)$$\n",
    "\n",
    "Now assume that $p(x)$ is a proper (normalized) probability mass function, then an easy way to sample from $p(x)$ is to:\n",
    "\n",
    " - lay out each possible outcome on a stick of length 1\n",
    " - sample a value $u$ from an uniform distribution\n",
    " - identify in which interval of the cumulative distribution function $u$ fall into and select and the associated $x_j$\n",
    "\n",
    "More formally, we can partition $F(x)$ in intervals:\n",
    "\n",
    "- $$ [\\Big(0, F(x_1) \\Big), \\Big(F(x_1), F(x_2) \\Big), ..., \\Big(F(x_k), 1 \\Big)]$$\n",
    "- Drawn $u \\sim Uniform(0, 1)$\n",
    "- Check in where interval $F(x_{i-1}) \\leq u \\leq F(x_i)$\n",
    "- Sample $F(x_i)$\n",
    "\n",
    "Such method can be easily extended to unnormalised discrete distribution, where instead to sampling $u$ from Uniform(0,1) we sample\n",
    "$u$ from $(0, F(X))$. Thus, we have:\n",
    "```python\n",
    "r = np.sum(b) * np.random().random()\n",
    "```\n",
    "Instead of:\n",
    "```python\n",
    "r = np.random().random()\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "This section is about unsupervised learning with text data.\n",
    "We'll be using a relatively small dataset, consisting of 3430 documents and a vocabulary of 6906 words drawn from the daily kos blog around 2004. \n",
    "\n",
    "You can download the data [here](https://s3-eu-west-1.amazonaws.com/lastmilecoding/exercise.tar.gz).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian mixture model\n",
    "\n",
    "We're going to model the documents as bags of words, with a bayesian mixture model.\n",
    "The documents are modeled using $K$ topics.\n",
    "The assignment of a document to a topic is modeled by the latent variable $z_d$.\n",
    "\n",
    "The topics are drawn from a categorical distribution with parameters $\\theta$, where $\\theta$ is drawn from a Dirichlet prior with parameter $\\alpha$.\n",
    "\n",
    "Each topic specifies a categorical distribution over words. The prior on each of these distributions is a Dirichlet with parameter $\\gamma$. \n",
    "\n",
    "\n",
    "The figure below shows this in a graphical model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Graphical Model](http://i.imgur.com/AAGnKZ7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, let's denote with $y_n$ the observations (i.e. the documents we see in the corpus).\n",
    "The conditional likelihood is:\n",
    "\n",
    "$$p(y_n|z_n \\negmedspace = \\negmedspace k,\\beta) = p(y_n|\\beta_k) = p(y_n|\\beta_{z_n})$$\n",
    "\n",
    "And we have a prior:\n",
    "\n",
    "$$p(\\beta_k)$$\n",
    "\n",
    "And a (latent) categorical assignment probability:\n",
    "\n",
    "$$p(z_n \\negmedspace= \\negmedspace k|\\theta) = \\theta_k$$\n",
    "\n",
    "which has a Dirichlet prior:\n",
    "\n",
    "$$p(\\theta|\\alpha) = Dir(\\alpha)$$\n",
    "\n",
    "Which gives our latent posterior:\n",
    "\n",
    "$$ p(z_n\\negmedspace=\\negmedspace k|y_n,\\theta,\\beta) \\quad \\propto p(z_n\\negmedspace =\\negmedspace k|\\theta)p(y_n|z_n \\negmedspace = \\negmedspace k,\\beta) \\quad \\propto \\theta_k p(y_n|\\beta_{z_n}) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gibbs Sampling\n",
    "We will explore this model by drawing from the posterior using MCMC, specifically Gibbs sampling.\n",
    "\n",
    "We will alternately sample the three types of variables, & iterate this procedure multiple times.\n",
    "\n",
    "First we'll sample the component parameters:\n",
    "\n",
    "$$p(\\beta_k|y,z) \\quad \\propto p(\\beta_k) \\prod_{n:z_n=k} p(y_n|\\beta_k) $$\n",
    "\n",
    "Then the latent allocations of documents to topics:\n",
    "$$ p(z_n \\negmedspace= \\negmedspace k|y_n,\\theta,\\beta) \\quad \\propto \\theta_k p(y_n|\\beta_{z_n})$$\n",
    "\n",
    "and then the mixing proportions:\n",
    "\n",
    "$$p(\\theta|z,\\alpha) = p(\\theta|\\alpha)p(z|\\theta) = \\mathrm{Dir}\\left(\\frac{c_k+\\alpha_k}{\\sum_j c_j + \\alpha_j}\\right)$$\n",
    "\n",
    "where $c_k$ are the counts for mixture component $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Collapsed Gibbs Sampler\n",
    "\n",
    "We marginalise over $\\theta$. (You do not need to derive this result).\n",
    "N.B. the notation $c_{-n}$ indicates all indices _except_ $n$. \n",
    "\n",
    "$$p(z_n \\negmedspace= \\negmedspace k|z_{-n},\\alpha) = \\frac{\\alpha + c_{-n,k}}{\\sum_j \\alpha + c_{-n,j}}$$\n",
    "\n",
    "which gives the _collapsed_ gibbs sampler for the latent assignments:\n",
    "\n",
    "$$p(z_n \\negmedspace= \\negmedspace k|y_n,z_{-n},\\beta,\\alpha) \\propto p(y_n|\\beta_k) \\frac{\\alpha + c_{-n,k}}{\\sum_j \\alpha + c_{-n,j}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "\n",
    "The documents have been split into two corpora, `A` and `B`.\n",
    "\n",
    "The array `words` is a list of all the words in both corpora.\n",
    "The matrices `A` and `B` are the train and test corpora, respectively. \n",
    "Each has 3 columns, there is one row for each unique word in each document.\n",
    "\n",
    "The first column is the document index, second is the word index (corresponding to `words`), and the third is the number of times that word appears in the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.load('data/mat_A.npy')\n",
    "B = np.load('data/mat_B.npy')\n",
    "words = np.load('data/words.npy')\n",
    "\n",
    "W = np.max(np.hstack((A[:,1],B[:,1]))) + 1   # number of unique words\n",
    "D = np.max(A[:,0]) + 1   # number of documents in A\n",
    "K = 20 # number of mixture components we will use\n",
    "\n",
    "alpha = 10  # parameter of the Dirichlet over mixture components\n",
    "gamma = 0.1 # parameter of the Dirichlet over words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below defines the following matrices:\n",
    " * `sd` : the mixture component assignment of each document\n",
    " * `swk` : K multinomial distributions over W unique words\n",
    " * `sk_docs` : the number of documents assigned to each mixture component\n",
    " * `sk_words` : the number of words assigned to mix component `k` accross all docs\n",
    " \n",
    "These are initialised by assigning each document to a mixture component at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    sd = np.floor(K*np.random.random((D,1))).astype(int)   \n",
    "    swk = np.zeros((W,K))              \n",
    "    sk_docs = np.zeros((K,1)) \n",
    "\n",
    "    for d in np.arange(D): \n",
    "        w = A[A[:,0]==d,1]      # unique words in doc d\n",
    "        c = A[A[:,0]==d,2]      # counts\n",
    "        k = sd[d]               # doc d is in mixture k\n",
    "        swk[w,k] = swk[w,k] + c # num times word w is assigned to mixture component k\n",
    "        sk_docs[k] = sk_docs[k] + 1\n",
    "\n",
    "    sk_words = np.sum(swk,axis=0).T\n",
    "    return sd, swk, sk_docs, sk_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code starts from this initial state & then performs a number of gibbs sampling sweeps. \n",
    "We will use the collapsed Gibbs sampler, which uses the trick of excluding the current document's counts before calculating the posterior and resampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gibbs sweep : 0\n",
      "gibbs sweep : 1\n",
      "gibbs sweep : 2\n",
      "gibbs sweep : 3\n",
      "gibbs sweep : 4\n",
      "gibbs sweep : 5\n",
      "gibbs sweep : 6\n",
      "gibbs sweep : 7\n",
      "gibbs sweep : 8\n",
      "gibbs sweep : 9\n"
     ]
    }
   ],
   "source": [
    "sd, swk, sk_docs, sk_words = init()\n",
    "# This makes a number of Gibbs sampling sweeps through all docs and words\n",
    "num_sweeps = 10\n",
    "for i_sweep in np.arange(num_sweeps): \n",
    "    print(\"gibbs sweep : {0}\".format(i_sweep))\n",
    "    for d in np.arange(D):\n",
    "        w = A[A[:,0]==d,1]      # unique words in doc d\n",
    "        c = A[A[:,0]==d,2]      # counts\n",
    "\n",
    "        # remove doc d's contributions from count tables\n",
    "        swk[w,sd[d]] = swk[w,sd[d]] - c \n",
    "        sk_docs[sd[d]] = sk_docs[sd[d]] - 1 \n",
    "        sk_words[sd[d]] = sk_words[sd[d]] - np.sum(c) \n",
    "        \n",
    "        # log probability of doc d under each mixture component\n",
    "        lb = np.zeros(K)    \n",
    "        for k in np.arange(K):\n",
    "            ll = np.dot(c,( np.log(swk[w,k]+gamma) - np.log(sk_words[k] + gamma*W) ))\n",
    "            lb[k] = np.log(sk_docs[k] + alpha) + ll\n",
    "\n",
    "        # assign doc d to a new component\n",
    "        b = np.exp(lb-np.max(lb))  \n",
    "        kk = sample_discrete(b)  \n",
    "\n",
    "        # add back doc d's contributions from count tables\n",
    "        swk[w,kk] = swk[w,kk] + c \n",
    "        sk_docs[kk] = sk_docs[kk] + 1 \n",
    "        sk_words[kk] = sk_words[kk] + np.sum(c)\n",
    "        sd[d] = kk"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2.1\n",
    "\n",
    "The code above is divided in:\n",
    "- a first part that define the model hyper-parameters. The most important are ``K, alpha`` and ``gamma``.\n",
    "- we randomly assign each document to a possible topic (```sd = np.floor(K*np.random.random((D,1))).astype(int)```)\n",
    "- based on the document assignment, it is possible to randomly initialize the counting matrixes\n",
    "\n",
    "\n",
    "Once all the needed data structure are randomly initialized, it is time to perform multiple steps of gibbs sampling to approximate the posterior distribution.\n",
    "Specifically, for each document ``d`` in my training set:\n",
    "- we exclude ``d`` from the current counting\n",
    "- we compute a new document-topic assignment based on the updated counting $$p(z_n \\negmedspace= \\negmedspace k|y_n,z_{-n},\\beta,\\alpha) \\propto p(y_n|\\beta_k) \\frac{\\alpha + c_{-n,k}}{\\sum_j \\alpha + c_{-n,j}}$$\n",
    "- we assign document ``d`` to a new topic based on the (log) probability previously computed\n",
    "- update the counting of our model according to the updated topic assignment\n",
    "\n",
    "\n",
    "I'm not really a fan of such model:\n",
    "- first of all, I would suggest a different parameter initialization. ``alpha = 10`` assume that a document might be associated to random topics (I have not tested it with different values, but usually model like LDA use an ``alpha < 1``).\n",
    "Topic models are usually adopted as an unsupervised method to do document classification/clustering over it's topic.\n",
    "Thus, we would like to have document associated to a relevant topic.\n",
    "``gamma = 0.1`` seems a common prior for topic-word distribution.\n",
    "- As LDA, even this mixture model does not consider the words' order, thus has a limited understanding of the semantic of each document. A bi-gram model could highly prevent such issue.\n",
    "- Bayesian mixture models are highly sensitive to the noise present in each document. Thus, it require careful preprocessing such as: stopword removal, stemming (seems that you haven't use stemming nor language check).\n",
    "- On the one had, LDA is not easily applicable to social media content such as twitter, where short documents (tweets) does not present significant word co-occurence. On the other hand, this simpler model might overcome the sparisity issue associated to short text since the topic distribution is sampled once at corpus level.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3684.9094661054687\n"
     ]
    }
   ],
   "source": [
    "def log_ll(B, d, swk, sk_docs, sk_words, alpha, gamma, W, K):\n",
    "    w = B[B[:, 0] == d + 2000, 1]\n",
    "    c = B[B[:, 0] == d + 2000, 2]\n",
    "\n",
    "    t = np.zeros([K, W])\n",
    "    for k in np.arange(K):\n",
    "        th = (sk_docs[k] + alpha) / np.sum(sk_docs + alpha)\n",
    "        ph = (swk[:, k] + gamma) / (sk_words[k] + gamma * W)\n",
    "        t[k] = th * ph\n",
    "\n",
    "    p = c * np.log(t[:, w].sum(0))\n",
    "    return p.sum(0)\n",
    "\n",
    "print(log_ll(B, 0, swk, sk_docs, sk_words, alpha, gamma, W, K))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2.2\n",
    "\n",
    "The log-probability describe how probable is that the words in a given document have been generated by the topic-word distributions learned on the training set.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2698.023289834625\n"
     ]
    }
   ],
   "source": [
    "def perplexity(B, swk, sk_docs, sk_words, alpha, gamma, W, K):\n",
    "    t = np.zeros([K, W])\n",
    "    for k in np.arange(K):\n",
    "        th = (sk_docs[k] + alpha) / np.sum(sk_docs + alpha)\n",
    "        ph = (swk[:, k] + gamma) / (sk_words[k] + gamma * W)\n",
    "        t[k] = th * ph\n",
    "\n",
    "    p = 0\n",
    "    for w in np.unique(B[:, 1]):\n",
    "        c = B[B[:, 1] == w, 2].sum()\n",
    "\n",
    "        p += c * np.log(t[:, w].sum(0))\n",
    "\n",
    "\n",
    "    p = p / B[:, 2].sum(0)\n",
    "\n",
    "    return np.exp(-p)\n",
    "\n",
    "print(perplexity(B,swk, sk_docs, sk_words, alpha, gamma, W, K))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2.3\n",
    "\n",
    "Perplexity define how much our model is surprised to see the test set.\n",
    "A smaller perplexity means the fitted model can better explain our test set.\n",
    "However, I don't really like perplexity to compare different language model because it is somehow dependant on the vocabulary size.\n",
    "Thus, the preprocessing and the tokenization might make model comparison NOT straightforward.\n",
    "\n",
    "Yet, in some cases, perplexity is an efficient way to estimate the log-likelihood of a document in the test set.\n",
    "Specifically, perplexity is defined as:\n",
    "\n",
    "$$ p(W_{test} | \\mathbf{M}) = \\exp - \\Big( \\frac{ \\sum_{w \\in W_{test}} \\log p(w|\\mathbf{M}) } { \\sum_{w \\in W_{test}} n^w } \\Big) $$\n",
    "\n",
    "where $ \\mathbf{M}$ is the trained model, $W_{test}$ are the words in the test set and $n^w$ stand for the number of times word $w$ appears in the test set.\n",
    "\n",
    "Note that, I have computed $\\log p(w|\\mathbf{M})$ as:\n",
    "\n",
    "$$  \\log p(w|\\mathbf{M}) = n^w \\log \\big ( \\sum_{k=1}^K  \\phi_{k,w} \\theta_k \\big ) $$\n",
    "\n",
    "No, the value that we got is not good at all."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3:\n",
    "Free form. Extend and/or analyse the above model, or use your favourite algorithm to find something interesting in this dataset. Show us what you can do!\n",
    "\n",
    "\n",
    "Here you can find a minimal implementation of the [GSC](https://arxiv.org/pdf/1706.00359.pdf) model: a topic language model based on variational inference.\n",
    "It is similar to a VAE, but the extractor network $q(\\theta|d)$ is used to extract the topic of a document (as softmax of the sampled latent space).\n",
    "Whereas the generator network $p(x|\\theta)$ is a simple MLP that generate the bag-of-word representation of a document from a single topic distribution $\\theta$.\n",
    "\n",
    "P.S. the code is inspired by the [NDM](https://github.com/YongfeiYan/Neural-Document-Modeling) library.\n",
    "Finally, you need to manyally create the following directories before run the model:\n",
    " - export_dir: save the trained model\n",
    " - runs: contains the tensorboard logs\n",
    " - data: contains the datasets\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\t | train_ppx 6379.76270 | train_ppx_doc 6394.93018 | train_loss 1196.05188 | train_loss_rec 1190.44580 | train_kld 0.58819 | train_penalty 0.10036 | train_penalty_mean 0.14354 | train_penalty_var 0.04318\n",
      "40\t | train_ppx 4920.72021 | train_ppx_doc 5037.79150 | train_loss 1160.65234 | train_loss_rec 1150.88013 | train_kld 4.85088 | train_penalty 0.09843 | train_penalty_mean 0.14178 | train_penalty_var 0.04335\n",
      "60\t | train_ppx 3575.73755 | train_ppx_doc 3754.04028 | train_loss 1117.21289 | train_loss_rec 1101.53857 | train_kld 10.78612 | train_penalty 0.09776 | train_penalty_mean 0.14122 | train_penalty_var 0.04346\n",
      "80\t | train_ppx 3067.97583 | train_ppx_doc 3173.77002 | train_loss 1096.31006 | train_loss_rec 1084.64722 | train_kld 6.85659 | train_penalty 0.09612 | train_penalty_mean 0.13981 | train_penalty_var 0.04369\n",
      "100\t | train_ppx 2895.11572 | train_ppx_doc 2968.43604 | train_loss 1088.31213 | train_loss_rec 1077.95032 | train_kld 5.66932 | train_penalty 0.09385 | train_penalty_mean 0.13785 | train_penalty_var 0.04400\n",
      "-->eval_ppx 2786.03711 | -->eval_ppx_doc 2869.49170 | -->eval_loss 1090.84375 | -->eval_loss_rec 1081.23938 | -->eval_kld 4.97461 | -->eval_penalty 0.09260 | -->eval_penalty_mean 0.13677 | -->eval_penalty_var 0.04417\n",
      "new best model\n",
      "120\t | train_ppx 2857.81421 | train_ppx_doc 2899.75928 | train_loss 1086.44019 | train_loss_rec 1076.87061 | train_kld 4.98599 | train_penalty 0.09167 | train_penalty_mean 0.13598 | train_penalty_var 0.04431\n",
      "140\t | train_ppx 2774.59180 | train_ppx_doc 2826.90991 | train_loss 1082.33167 | train_loss_rec 1073.32349 | train_kld 4.51543 | train_penalty 0.08985 | train_penalty_mean 0.13445 | train_penalty_var 0.04460\n",
      "160\t | train_ppx 2703.81787 | train_ppx_doc 2744.93433 | train_loss 1078.74683 | train_loss_rec 1069.52478 | train_kld 4.80125 | train_penalty 0.08842 | train_penalty_mean 0.13327 | train_penalty_var 0.04485\n",
      "180\t | train_ppx 2612.70337 | train_ppx_doc 2658.37256 | train_loss 1074.00049 | train_loss_rec 1065.09778 | train_kld 4.56812 | train_penalty 0.08669 | train_penalty_mean 0.13182 | train_penalty_var 0.04513\n",
      "200\t | train_ppx 2572.94897 | train_ppx_doc 2616.21631 | train_loss 1071.83777 | train_loss_rec 1063.16382 | train_kld 4.41755 | train_penalty 0.08513 | train_penalty_mean 0.13051 | train_penalty_var 0.04538\n",
      "-->eval_ppx 2476.84448 | -->eval_ppx_doc 2552.54272 | -->eval_loss 1074.31506 | -->eval_loss_rec 1066.27905 | -->eval_kld 3.82666 | -->eval_penalty 0.08418 | -->eval_penalty_mean 0.12970 | -->eval_penalty_var 0.04552\n",
      "new best model\n",
      "220\t | train_ppx 2544.11938 | train_ppx_doc 2572.52344 | train_loss 1070.22070 | train_loss_rec 1061.96606 | train_kld 4.08334 | train_penalty 0.08343 | train_penalty_mean 0.12908 | train_penalty_var 0.04565\n",
      "240\t | train_ppx 2498.92896 | train_ppx_doc 2531.30591 | train_loss 1067.68787 | train_loss_rec 1059.73987 | train_kld 3.87320 | train_penalty 0.08149 | train_penalty_mean 0.12745 | train_penalty_var 0.04596\n",
      "260\t | train_ppx 2477.31470 | train_ppx_doc 2510.68628 | train_loss 1066.40527 | train_loss_rec 1058.67395 | train_kld 3.75812 | train_penalty 0.07946 | train_penalty_mean 0.12573 | train_penalty_var 0.04627\n",
      "280\t | train_ppx 2419.80884 | train_ppx_doc 2457.44775 | train_loss 1063.12903 | train_loss_rec 1055.52222 | train_kld 3.71680 | train_penalty 0.07780 | train_penalty_mean 0.12431 | train_penalty_var 0.04651\n",
      "300\t | train_ppx 2437.66919 | train_ppx_doc 2462.55151 | train_loss 1064.04492 | train_loss_rec 1056.66663 | train_kld 3.57218 | train_penalty 0.07612 | train_penalty_mean 0.12286 | train_penalty_var 0.04674\n",
      "-->eval_ppx 2398.75317 | -->eval_ppx_doc 2452.39429 | -->eval_loss 1069.47363 | -->eval_loss_rec 1062.53406 | -->eval_kld 3.18489 | -->eval_penalty 0.07509 | -->eval_penalty_mean 0.12196 | -->eval_penalty_var 0.04686\n",
      "new best model\n",
      "320\t | train_ppx 2411.68555 | train_ppx_doc 2437.86450 | train_loss 1062.48828 | train_loss_rec 1055.19177 | train_kld 3.59007 | train_penalty 0.07413 | train_penalty_mean 0.12110 | train_penalty_var 0.04697\n",
      "340\t | train_ppx 2413.92896 | train_ppx_doc 2439.55054 | train_loss 1062.52441 | train_loss_rec 1055.19092 | train_kld 3.71766 | train_penalty 0.07232 | train_penalty_mean 0.11951 | train_penalty_var 0.04719\n",
      "360\t | train_ppx 2396.70996 | train_ppx_doc 2413.74609 | train_loss 1061.46826 | train_loss_rec 1054.39734 | train_kld 3.53773 | train_penalty 0.07066 | train_penalty_mean 0.11805 | train_penalty_var 0.04739\n",
      "380\t | train_ppx 2329.99756 | train_ppx_doc 2373.40039 | train_loss 1057.53760 | train_loss_rec 1050.92163 | train_kld 3.17553 | train_penalty 0.06881 | train_penalty_mean 0.11637 | train_penalty_var 0.04756\n",
      "400\t | train_ppx 2337.43335 | train_ppx_doc 2371.83911 | train_loss 1057.88391 | train_loss_rec 1051.33032 | train_kld 3.19994 | train_penalty 0.06707 | train_penalty_mean 0.11478 | train_penalty_var 0.04771\n",
      "-->eval_ppx 2347.63062 | -->eval_ppx_doc 2395.23975 | -->eval_loss 1066.06897 | -->eval_loss_rec 1059.59827 | -->eval_kld 3.17073 | -->eval_penalty 0.06600 | -->eval_penalty_mean 0.11379 | -->eval_penalty_var 0.04779\n",
      "new best model\n",
      "420\t | train_ppx 2328.30176 | train_ppx_doc 2354.53638 | train_loss 1057.26379 | train_loss_rec 1050.51282 | train_kld 3.48533 | train_penalty 0.06531 | train_penalty_mean 0.11315 | train_penalty_var 0.04784\n",
      "440\t | train_ppx 2301.70947 | train_ppx_doc 2334.67651 | train_loss 1055.62805 | train_loss_rec 1049.11511 | train_kld 3.32145 | train_penalty 0.06383 | train_penalty_mean 0.11177 | train_penalty_var 0.04794\n",
      "460\t | train_ppx 2301.92798 | train_ppx_doc 2323.79419 | train_loss 1055.57568 | train_loss_rec 1048.96277 | train_kld 3.48669 | train_penalty 0.06252 | train_penalty_mean 0.11053 | train_penalty_var 0.04801\n",
      "480\t | train_ppx 2290.95239 | train_ppx_doc 2309.07690 | train_loss 1054.84961 | train_loss_rec 1048.16821 | train_kld 3.63154 | train_penalty 0.06100 | train_penalty_mean 0.10908 | train_penalty_var 0.04808\n",
      "500\t | train_ppx 2265.65967 | train_ppx_doc 2287.72778 | train_loss 1053.27637 | train_loss_rec 1046.62463 | train_kld 3.66598 | train_penalty 0.05972 | train_penalty_mean 0.10785 | train_penalty_var 0.04813\n",
      "-->eval_ppx 2299.95532 | -->eval_ppx_doc 2335.85010 | -->eval_loss 1062.91309 | -->eval_loss_rec 1056.51868 | -->eval_kld 3.44072 | -->eval_penalty 0.05907 | -->eval_penalty_mean 0.10724 | -->eval_penalty_var 0.04816\n",
      "new best model\n",
      "520\t | train_ppx 2260.48096 | train_ppx_doc 2279.64673 | train_loss 1052.90442 | train_loss_rec 1046.13306 | train_kld 3.84637 | train_penalty 0.05850 | train_penalty_mean 0.10669 | train_penalty_var 0.04819\n",
      "540\t | train_ppx 2252.11597 | train_ppx_doc 2269.04907 | train_loss 1052.33337 | train_loss_rec 1045.77881 | train_kld 3.69674 | train_penalty 0.05716 | train_penalty_mean 0.10539 | train_penalty_var 0.04823\n",
      "560\t | train_ppx 2256.23267 | train_ppx_doc 2270.21582 | train_loss 1052.52295 | train_loss_rec 1045.82983 | train_kld 3.89384 | train_penalty 0.05599 | train_penalty_mean 0.10424 | train_penalty_var 0.04826\n",
      "580\t | train_ppx 2256.02930 | train_ppx_doc 2263.39526 | train_loss 1052.45544 | train_loss_rec 1045.78979 | train_kld 3.92156 | train_penalty 0.05489 | train_penalty_mean 0.10317 | train_penalty_var 0.04828\n",
      "600\t | train_ppx 2218.59619 | train_ppx_doc 2224.99097 | train_loss 1050.13171 | train_loss_rec 1043.52747 | train_kld 3.90930 | train_penalty 0.05390 | train_penalty_mean 0.10222 | train_penalty_var 0.04832\n",
      "-->eval_ppx 2244.38867 | -->eval_ppx_doc 2273.07007 | -->eval_loss 1059.28027 | -->eval_loss_rec 1052.93359 | -->eval_kld 3.67716 | -->eval_penalty 0.05339 | -->eval_penalty_mean 0.10173 | -->eval_penalty_var 0.04834\n",
      "new best model\n",
      "620\t | train_ppx 2191.96680 | train_ppx_doc 2205.13818 | train_loss 1048.43713 | train_loss_rec 1041.93774 | train_kld 3.85752 | train_penalty 0.05284 | train_penalty_mean 0.10121 | train_penalty_var 0.04837\n",
      "640\t | train_ppx 2205.76807 | train_ppx_doc 2216.12402 | train_loss 1049.23560 | train_loss_rec 1042.77051 | train_kld 3.87798 | train_penalty 0.05174 | train_penalty_mean 0.10015 | train_penalty_var 0.04841\n",
      "660\t | train_ppx 2191.69189 | train_ppx_doc 2203.06348 | train_loss 1048.31787 | train_loss_rec 1041.82739 | train_kld 3.95063 | train_penalty 0.05080 | train_penalty_mean 0.09923 | train_penalty_var 0.04843\n",
      "680\t | train_ppx 2160.06104 | train_ppx_doc 2174.83105 | train_loss 1046.29272 | train_loss_rec 1039.79456 | train_kld 4.00710 | train_penalty 0.04982 | train_penalty_mean 0.09827 | train_penalty_var 0.04845\n",
      "700\t | train_ppx 2167.11572 | train_ppx_doc 2178.34937 | train_loss 1046.69250 | train_loss_rec 1040.02222 | train_kld 4.22275 | train_penalty 0.04895 | train_penalty_mean 0.09740 | train_penalty_var 0.04845\n",
      "-->eval_ppx 2216.37769 | -->eval_ppx_doc 2244.38110 | -->eval_loss 1057.32068 | -->eval_loss_rec 1051.02820 | -->eval_kld 3.86270 | -->eval_penalty 0.04860 | -->eval_penalty_mean 0.09706 | -->eval_penalty_var 0.04846\n",
      "new best model\n",
      "720\t | train_ppx 2159.94360 | train_ppx_doc 2171.67847 | train_loss 1046.20630 | train_loss_rec 1039.55322 | train_kld 4.24116 | train_penalty 0.04824 | train_penalty_mean 0.09672 | train_penalty_var 0.04848\n",
      "740\t | train_ppx 2169.81641 | train_ppx_doc 2181.70386 | train_loss 1046.79321 | train_loss_rec 1040.16052 | train_kld 4.25362 | train_penalty 0.04758 | train_penalty_mean 0.09609 | train_penalty_var 0.04851\n",
      "760\t | train_ppx 2131.90869 | train_ppx_doc 2143.12036 | train_loss 1044.35974 | train_loss_rec 1037.77246 | train_kld 4.24577 | train_penalty 0.04683 | train_penalty_mean 0.09536 | train_penalty_var 0.04853\n",
      "780\t | train_ppx 2143.35962 | train_ppx_doc 2163.91553 | train_loss 1045.05591 | train_loss_rec 1038.25757 | train_kld 4.48889 | train_penalty 0.04619 | train_penalty_mean 0.09476 | train_penalty_var 0.04857\n",
      "800\t | train_ppx 2130.15479 | train_ppx_doc 2147.70361 | train_loss 1044.19189 | train_loss_rec 1037.61792 | train_kld 4.28843 | train_penalty 0.04571 | train_penalty_mean 0.09432 | train_penalty_var 0.04861\n",
      "-->eval_ppx 2192.71533 | -->eval_ppx_doc 2221.40771 | -->eval_loss 1055.68616 | -->eval_loss_rec 1049.41272 | -->eval_kld 4.00835 | -->eval_penalty 0.04530 | -->eval_penalty_mean 0.09392 | -->eval_penalty_var 0.04861\n",
      "new best model\n",
      "820\t | train_ppx 2136.07886 | train_ppx_doc 2143.39941 | train_loss 1044.53809 | train_loss_rec 1037.76196 | train_kld 4.52174 | train_penalty 0.04509 | train_penalty_mean 0.09371 | train_penalty_var 0.04863\n",
      "840\t | train_ppx 2117.16577 | train_ppx_doc 2132.45679 | train_loss 1043.31055 | train_loss_rec 1036.72583 | train_kld 4.34881 | train_penalty 0.04472 | train_penalty_mean 0.09336 | train_penalty_var 0.04864\n",
      "860\t | train_ppx 2095.28369 | train_ppx_doc 2111.88550 | train_loss 1041.88269 | train_loss_rec 1035.13049 | train_kld 4.53198 | train_penalty 0.04441 | train_penalty_mean 0.09308 | train_penalty_var 0.04868\n",
      "880\t | train_ppx 2102.02051 | train_ppx_doc 2121.20264 | train_loss 1042.28821 | train_loss_rec 1035.60583 | train_kld 4.49298 | train_penalty 0.04379 | train_penalty_mean 0.09253 | train_penalty_var 0.04875\n",
      "900\t | train_ppx 2103.35815 | train_ppx_doc 2112.39795 | train_loss 1042.34546 | train_loss_rec 1035.40137 | train_kld 4.78378 | train_penalty 0.04320 | train_penalty_mean 0.09198 | train_penalty_var 0.04878\n",
      "-->eval_ppx 2175.73120 | -->eval_ppx_doc 2204.47681 | -->eval_loss 1054.50049 | -->eval_loss_rec 1048.21106 | -->eval_kld 4.14545 | -->eval_penalty 0.04288 | -->eval_penalty_mean 0.09169 | -->eval_penalty_var 0.04880\n",
      "new best model\n",
      "920\t | train_ppx 2081.03516 | train_ppx_doc 2106.10083 | train_loss 1040.86670 | train_loss_rec 1034.02161 | train_kld 4.71313 | train_penalty 0.04264 | train_penalty_mean 0.09148 | train_penalty_var 0.04883\n",
      "940\t | train_ppx 2054.64160 | train_ppx_doc 2077.61060 | train_loss 1039.10059 | train_loss_rec 1032.02686 | train_kld 4.97251 | train_penalty 0.04202 | train_penalty_mean 0.09091 | train_penalty_var 0.04889\n",
      "960\t | train_ppx 2066.08813 | train_ppx_doc 2083.27710 | train_loss 1039.81458 | train_loss_rec 1033.21936 | train_kld 4.53538 | train_penalty 0.04120 | train_penalty_mean 0.09014 | train_penalty_var 0.04895\n",
      "980\t | train_ppx 2063.80981 | train_ppx_doc 2087.33398 | train_loss 1039.62476 | train_loss_rec 1033.02234 | train_kld 4.58234 | train_penalty 0.04040 | train_penalty_mean 0.08938 | train_penalty_var 0.04898\n",
      "1000\t | train_ppx 2061.48950 | train_ppx_doc 2081.50366 | train_loss 1039.43433 | train_loss_rec 1032.64368 | train_kld 4.80832 | train_penalty 0.03965 | train_penalty_mean 0.08868 | train_penalty_var 0.04903\n",
      "-->eval_ppx 2157.03174 | -->eval_ppx_doc 2183.33984 | -->eval_loss 1053.13794 | -->eval_loss_rec 1046.93506 | -->eval_kld 4.23920 | -->eval_penalty 0.03927 | -->eval_penalty_mean 0.08833 | -->eval_penalty_var 0.04905\n",
      "new best model\n",
      "1020\t | train_ppx 2071.57910 | train_ppx_doc 2086.05225 | train_loss 1040.06348 | train_loss_rec 1033.04138 | train_kld 5.07437 | train_penalty 0.03896 | train_penalty_mean 0.08804 | train_penalty_var 0.04908\n",
      "1040\t | train_ppx 2048.94067 | train_ppx_doc 2067.76465 | train_loss 1038.53552 | train_loss_rec 1031.61658 | train_kld 5.00518 | train_penalty 0.03827 | train_penalty_mean 0.08739 | train_penalty_var 0.04911\n",
      "1060\t | train_ppx 2046.81189 | train_ppx_doc 2068.57349 | train_loss 1038.35974 | train_loss_rec 1031.55945 | train_kld 4.92113 | train_penalty 0.03759 | train_penalty_mean 0.08672 | train_penalty_var 0.04913\n",
      "1080\t | train_ppx 2026.29651 | train_ppx_doc 2048.65259 | train_loss 1036.95740 | train_loss_rec 1029.99438 | train_kld 5.11664 | train_penalty 0.03693 | train_penalty_mean 0.08609 | train_penalty_var 0.04916\n",
      "1100\t | train_ppx 2033.11694 | train_ppx_doc 2066.28320 | train_loss 1037.38416 | train_loss_rec 1030.52295 | train_kld 5.04471 | train_penalty 0.03633 | train_penalty_mean 0.08551 | train_penalty_var 0.04918\n",
      "-->eval_ppx 2154.92334 | -->eval_ppx_doc 2182.09497 | -->eval_loss 1052.83765 | -->eval_loss_rec 1046.75757 | -->eval_kld 4.28302 | -->eval_penalty 0.03594 | -->eval_penalty_mean 0.08514 | -->eval_penalty_var 0.04920\n",
      "new best model\n",
      "1120\t | train_ppx 2018.87708 | train_ppx_doc 2043.20679 | train_loss 1036.39490 | train_loss_rec 1029.70593 | train_kld 4.90637 | train_penalty 0.03565 | train_penalty_mean 0.08486 | train_penalty_var 0.04921\n",
      "1140\t | train_ppx 2019.09082 | train_ppx_doc 2046.73682 | train_loss 1036.38013 | train_loss_rec 1029.56787 | train_kld 5.05863 | train_penalty 0.03507 | train_penalty_mean 0.08428 | train_penalty_var 0.04921\n",
      "1160\t | train_ppx 2020.04126 | train_ppx_doc 2043.91821 | train_loss 1036.41772 | train_loss_rec 1029.56165 | train_kld 5.12910 | train_penalty 0.03454 | train_penalty_mean 0.08377 | train_penalty_var 0.04923\n",
      "1180\t | train_ppx 2003.78503 | train_ppx_doc 2030.80603 | train_loss 1035.29285 | train_loss_rec 1028.49451 | train_kld 5.09767 | train_penalty 0.03401 | train_penalty_mean 0.08325 | train_penalty_var 0.04924\n",
      "1200\t | train_ppx 2002.47363 | train_ppx_doc 2024.94141 | train_loss 1035.17639 | train_loss_rec 1028.26050 | train_kld 5.24260 | train_penalty 0.03347 | train_penalty_mean 0.08271 | train_penalty_var 0.04924\n",
      "-->eval_ppx 2146.47705 | -->eval_ppx_doc 2167.71509 | -->eval_loss 1052.16077 | -->eval_loss_rec 1045.86902 | -->eval_kld 4.63358 | -->eval_penalty 0.03317 | -->eval_penalty_mean 0.08241 | -->eval_penalty_var 0.04924\n",
      "new best model\n",
      "1220\t | train_ppx 1989.90442 | train_ppx_doc 2016.88721 | train_loss 1034.29224 | train_loss_rec 1027.23755 | train_kld 5.40955 | train_penalty 0.03290 | train_penalty_mean 0.08215 | train_penalty_var 0.04925\n",
      "1240\t | train_ppx 2011.49915 | train_ppx_doc 2032.41516 | train_loss 1035.73657 | train_loss_rec 1028.80701 | train_kld 5.30749 | train_penalty 0.03244 | train_penalty_mean 0.08170 | train_penalty_var 0.04926\n",
      "1260\t | train_ppx 2010.17883 | train_ppx_doc 2031.95386 | train_loss 1035.62952 | train_loss_rec 1028.61279 | train_kld 5.41239 | train_penalty 0.03208 | train_penalty_mean 0.08136 | train_penalty_var 0.04928\n",
      "1280\t | train_ppx 1996.51379 | train_ppx_doc 2023.97705 | train_loss 1034.68164 | train_loss_rec 1027.53564 | train_kld 5.56214 | train_penalty 0.03168 | train_penalty_mean 0.08096 | train_penalty_var 0.04929\n",
      "1300\t | train_ppx 1994.47656 | train_ppx_doc 2023.42798 | train_loss 1034.51697 | train_loss_rec 1027.78992 | train_kld 5.16921 | train_penalty 0.03116 | train_penalty_mean 0.08043 | train_penalty_var 0.04927\n",
      "-->eval_ppx 2138.45630 | -->eval_ppx_doc 2163.81030 | -->eval_loss 1051.53406 | -->eval_loss_rec 1045.10266 | -->eval_kld 4.88736 | -->eval_penalty 0.03088 | -->eval_penalty_mean 0.08015 | -->eval_penalty_var 0.04927\n",
      "new best model\n",
      "1320\t | train_ppx 1984.85059 | train_ppx_doc 2005.82422 | train_loss 1033.83752 | train_loss_rec 1026.72217 | train_kld 5.57914 | train_penalty 0.03072 | train_penalty_mean 0.07999 | train_penalty_var 0.04927\n",
      "1340\t | train_ppx 1973.88513 | train_ppx_doc 2006.26514 | train_loss 1033.06641 | train_loss_rec 1025.90210 | train_kld 5.64614 | train_penalty 0.03036 | train_penalty_mean 0.07962 | train_penalty_var 0.04926\n",
      "1360\t | train_ppx 1981.70703 | train_ppx_doc 2013.07373 | train_loss 1033.58875 | train_loss_rec 1026.35864 | train_kld 5.72731 | train_penalty 0.03006 | train_penalty_mean 0.07930 | train_penalty_var 0.04924\n",
      "1380\t | train_ppx 1982.09827 | train_ppx_doc 2005.75916 | train_loss 1033.59949 | train_loss_rec 1026.32166 | train_kld 5.79093 | train_penalty 0.02973 | train_penalty_mean 0.07899 | train_penalty_var 0.04925\n",
      "1400\t | train_ppx 1986.73584 | train_ppx_doc 2009.51367 | train_loss 1033.90601 | train_loss_rec 1026.71643 | train_kld 5.71409 | train_penalty 0.02951 | train_penalty_mean 0.07877 | train_penalty_var 0.04926\n",
      "-->eval_ppx 2134.99316 | -->eval_ppx_doc 2158.98389 | -->eval_loss 1051.23730 | -->eval_loss_rec 1044.62146 | -->eval_kld 5.14655 | -->eval_penalty 0.02939 | -->eval_penalty_mean 0.07865 | -->eval_penalty_var 0.04926\n",
      "new best model\n",
      "1420\t | train_ppx 1971.91235 | train_ppx_doc 1998.19568 | train_loss 1032.87329 | train_loss_rec 1025.64673 | train_kld 5.76568 | train_penalty 0.02922 | train_penalty_mean 0.07848 | train_penalty_var 0.04926\n",
      "1440\t | train_ppx 1978.80530 | train_ppx_doc 1996.58984 | train_loss 1033.33179 | train_loss_rec 1026.12476 | train_kld 5.76176 | train_penalty 0.02890 | train_penalty_mean 0.07818 | train_penalty_var 0.04927\n",
      "1460\t | train_ppx 1968.12756 | train_ppx_doc 1997.52600 | train_loss 1032.58020 | train_loss_rec 1025.40845 | train_kld 5.74268 | train_penalty 0.02858 | train_penalty_mean 0.07786 | train_penalty_var 0.04928\n",
      "1480\t | train_ppx 1973.75525 | train_ppx_doc 1997.74512 | train_loss 1032.95618 | train_loss_rec 1025.83606 | train_kld 5.70309 | train_penalty 0.02834 | train_penalty_mean 0.07761 | train_penalty_var 0.04927\n",
      "1500\t | train_ppx 1948.83484 | train_ppx_doc 1976.75317 | train_loss 1031.21558 | train_loss_rec 1024.32959 | train_kld 5.48223 | train_penalty 0.02808 | train_penalty_mean 0.07734 | train_penalty_var 0.04926\n",
      "-->eval_ppx 2123.77954 | -->eval_ppx_doc 2148.37354 | -->eval_loss 1050.44324 | -->eval_loss_rec 1044.28784 | -->eval_kld 4.75917 | -->eval_penalty 0.02793 | -->eval_penalty_mean 0.07719 | -->eval_penalty_var 0.04926\n",
      "new best model\n",
      "1520\t | train_ppx 1954.76331 | train_ppx_doc 1973.93213 | train_loss 1031.61902 | train_loss_rec 1024.58191 | train_kld 5.64302 | train_penalty 0.02788 | train_penalty_mean 0.07714 | train_penalty_var 0.04926\n",
      "1540\t | train_ppx 1951.09058 | train_ppx_doc 1979.40271 | train_loss 1031.34839 | train_loss_rec 1024.24207 | train_kld 5.72712 | train_penalty 0.02758 | train_penalty_mean 0.07685 | train_penalty_var 0.04927\n",
      "1560\t | train_ppx 1927.00562 | train_ppx_doc 1955.67041 | train_loss 1029.64429 | train_loss_rec 1022.33118 | train_kld 5.94939 | train_penalty 0.02727 | train_penalty_mean 0.07653 | train_penalty_var 0.04926\n",
      "1580\t | train_ppx 1967.18835 | train_ppx_doc 1988.26172 | train_loss 1032.43689 | train_loss_rec 1025.32898 | train_kld 5.75737 | train_penalty 0.02701 | train_penalty_mean 0.07625 | train_penalty_var 0.04924\n",
      "1600\t | train_ppx 1953.73914 | train_ppx_doc 1979.51318 | train_loss 1031.48755 | train_loss_rec 1024.09668 | train_kld 6.05693 | train_penalty 0.02668 | train_penalty_mean 0.07591 | train_penalty_var 0.04923\n",
      "-->eval_ppx 2122.99390 | -->eval_ppx_doc 2142.37158 | -->eval_loss 1050.32092 | -->eval_loss_rec 1043.86926 | -->eval_kld 5.12686 | -->eval_penalty 0.02650 | -->eval_penalty_mean 0.07572 | -->eval_penalty_var 0.04923\n",
      "new best model\n",
      "1620\t | train_ppx 1946.31152 | train_ppx_doc 1970.39905 | train_loss 1030.95215 | train_loss_rec 1023.81854 | train_kld 5.81720 | train_penalty 0.02633 | train_penalty_mean 0.07555 | train_penalty_var 0.04922\n",
      "1640\t | train_ppx 1946.10278 | train_ppx_doc 1968.45605 | train_loss 1030.92651 | train_loss_rec 1023.37701 | train_kld 6.24434 | train_penalty 0.02610 | train_penalty_mean 0.07532 | train_penalty_var 0.04921\n",
      "1660\t | train_ppx 1933.30933 | train_ppx_doc 1955.99500 | train_loss 1030.01245 | train_loss_rec 1022.65820 | train_kld 6.06640 | train_penalty 0.02576 | train_penalty_mean 0.07499 | train_penalty_var 0.04923\n",
      "1680\t | train_ppx 1922.43335 | train_ppx_doc 1949.91772 | train_loss 1029.23486 | train_loss_rec 1021.97491 | train_kld 5.98281 | train_penalty 0.02555 | train_penalty_mean 0.07480 | train_penalty_var 0.04925\n",
      "1700\t | train_ppx 1927.42566 | train_ppx_doc 1962.28955 | train_loss 1029.56848 | train_loss_rec 1022.08026 | train_kld 6.22985 | train_penalty 0.02517 | train_penalty_mean 0.07441 | train_penalty_var 0.04925\n",
      "-->eval_ppx 2113.23926 | -->eval_ppx_doc 2129.42163 | -->eval_loss 1049.61499 | -->eval_loss_rec 1043.15173 | -->eval_kld 5.21382 | -->eval_penalty 0.02499 | -->eval_penalty_mean 0.07423 | -->eval_penalty_var 0.04924\n",
      "new best model\n",
      "1720\t | train_ppx 1920.82056 | train_ppx_doc 1948.20850 | train_loss 1029.09106 | train_loss_rec 1021.78552 | train_kld 6.05800 | train_penalty 0.02495 | train_penalty_mean 0.07419 | train_penalty_var 0.04924\n",
      "1740\t | train_ppx 1920.09717 | train_ppx_doc 1948.05249 | train_loss 1029.02612 | train_loss_rec 1021.74408 | train_kld 6.04810 | train_penalty 0.02468 | train_penalty_mean 0.07393 | train_penalty_var 0.04926\n",
      "1760\t | train_ppx 1917.53162 | train_ppx_doc 1943.56360 | train_loss 1028.82812 | train_loss_rec 1021.63531 | train_kld 5.97531 | train_penalty 0.02435 | train_penalty_mean 0.07360 | train_penalty_var 0.04925\n",
      "1780\t | train_ppx 1918.20740 | train_ppx_doc 1940.65759 | train_loss 1028.86511 | train_loss_rec 1021.55316 | train_kld 6.10513 | train_penalty 0.02413 | train_penalty_mean 0.07338 | train_penalty_var 0.04924\n",
      "1800\t | train_ppx 1900.86182 | train_ppx_doc 1938.22998 | train_loss 1027.61414 | train_loss_rec 1020.10748 | train_kld 6.31592 | train_penalty 0.02382 | train_penalty_mean 0.07305 | train_penalty_var 0.04924\n",
      "-->eval_ppx 2105.23950 | -->eval_ppx_doc 2120.19556 | -->eval_loss 1049.03345 | -->eval_loss_rec 1042.37939 | -->eval_kld 5.46671 | -->eval_penalty 0.02374 | -->eval_penalty_mean 0.07296 | -->eval_penalty_var 0.04922\n",
      "new best model\n",
      "1820\t | train_ppx 1909.68335 | train_ppx_doc 1935.89685 | train_loss 1028.23596 | train_loss_rec 1020.74249 | train_kld 6.31049 | train_penalty 0.02366 | train_penalty_mean 0.07289 | train_penalty_var 0.04923\n",
      "1840\t | train_ppx 1906.31165 | train_ppx_doc 1932.30652 | train_loss 1027.98389 | train_loss_rec 1020.28796 | train_kld 6.52488 | train_penalty 0.02342 | train_penalty_mean 0.07266 | train_penalty_var 0.04924\n",
      "1860\t | train_ppx 1912.82483 | train_ppx_doc 1939.98132 | train_loss 1028.43909 | train_loss_rec 1020.87708 | train_kld 6.39929 | train_penalty 0.02325 | train_penalty_mean 0.07249 | train_penalty_var 0.04924\n",
      "1880\t | train_ppx 1876.15222 | train_ppx_doc 1907.67566 | train_loss 1025.79028 | train_loss_rec 1018.54938 | train_kld 6.09544 | train_penalty 0.02291 | train_penalty_mean 0.07215 | train_penalty_var 0.04923\n",
      "1900\t | train_ppx 1898.85608 | train_ppx_doc 1919.70630 | train_loss 1027.41028 | train_loss_rec 1019.92090 | train_kld 6.35908 | train_penalty 0.02261 | train_penalty_mean 0.07184 | train_penalty_var 0.04923\n",
      "-->eval_ppx 2098.63232 | -->eval_ppx_doc 2112.15332 | -->eval_loss 1048.53992 | -->eval_loss_rec 1042.11707 | -->eval_kld 5.29884 | -->eval_penalty 0.02248 | -->eval_penalty_mean 0.07172 | -->eval_penalty_var 0.04923\n",
      "new best model\n",
      "1920\t | train_ppx 1883.43323 | train_ppx_doc 1915.63525 | train_loss 1026.29199 | train_loss_rec 1018.82764 | train_kld 6.34370 | train_penalty 0.02241 | train_penalty_mean 0.07164 | train_penalty_var 0.04923\n",
      "1940\t | train_ppx 1874.20825 | train_ppx_doc 1904.39014 | train_loss 1025.61536 | train_loss_rec 1017.99390 | train_kld 6.50983 | train_penalty 0.02223 | train_penalty_mean 0.07146 | train_penalty_var 0.04923\n",
      "1960\t | train_ppx 1875.83997 | train_ppx_doc 1904.79333 | train_loss 1025.72205 | train_loss_rec 1018.15479 | train_kld 6.46743 | train_penalty 0.02199 | train_penalty_mean 0.07122 | train_penalty_var 0.04922\n",
      "1980\t | train_ppx 1898.01062 | train_ppx_doc 1926.18433 | train_loss 1027.31482 | train_loss_rec 1019.54169 | train_kld 6.67745 | train_penalty 0.02191 | train_penalty_mean 0.07113 | train_penalty_var 0.04922\n",
      "2000\t | train_ppx 1871.13208 | train_ppx_doc 1900.45947 | train_loss 1025.36096 | train_loss_rec 1017.62488 | train_kld 6.65539 | train_penalty 0.02161 | train_penalty_mean 0.07083 | train_penalty_var 0.04922\n",
      "-->eval_ppx 2090.76562 | -->eval_ppx_doc 2103.69312 | -->eval_loss 1047.97473 | -->eval_loss_rec 1041.32263 | -->eval_kld 5.57897 | -->eval_penalty 0.02147 | -->eval_penalty_mean 0.07068 | -->eval_penalty_var 0.04921\n",
      "new best model\n"
     ]
    }
   ],
   "source": [
    "from src.task import TopicModelTask\n",
    "from src.components import Conf, GSM\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from src.dataset import load_news_data, load_kos_data\n",
    "from collections import namedtuple\n",
    "from os import path\n",
    "\n",
    "ARGS = namedtuple(\"ARGS\", [\"lr\", \"epochs\", \"evaluate_every\", \"batch_size\", \"data_dir\", \"runs_dir\", \"export_dir\", \"device\"])\n",
    "args = ARGS(\n",
    "    lr=0.001,\n",
    "    epochs=100,\n",
    "    evaluate_every=5,\n",
    "    batch_size=100,\n",
    "    data_dir=\"data\",\n",
    "    runs_dir=\"runs\",\n",
    "    export_dir=\"export_dir\",\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "def write_summary(writer, stat, step, prefix='train'):\n",
    "    stat = stat.get_dict()\n",
    "    for k, v in stat.items():\n",
    "        writer.add_scalars(k, {prefix: v}, global_step=step)\n",
    "\n",
    "\n",
    "def save_checkpoint(args, model):\n",
    "    filename = path.join(args.export_dir, 'model_best.pt')\n",
    "    state_dict = model.to(\"cpu\").state_dict()\n",
    "    torch.save(state_dict, filename)\n",
    "    model.to(torch.device(args.device))\n",
    "\n",
    "\n",
    "def save_topics(args, vocab, topic_prob, epoch, topk=100):\n",
    "    topic_prob = topic_prob.detach()\n",
    "    values, indices = torch.topk(topic_prob, k=topk, dim=-1)\n",
    "\n",
    "    topics = []\n",
    "    for t in indices:\n",
    "        topics.append(' '.join([vocab.get_word(i.item()) for i in t]))\n",
    "\n",
    "\n",
    "    with open(path.join(args.export_dir, \"topic-{}.topics\".format(epoch)), 'w') as f:\n",
    "        f.write('\\n'.join(topics))\n",
    "\n",
    "    str_values = []\n",
    "    for t in values:\n",
    "        str_values.append(' '.join([str(v) for v in t]))\n",
    "\n",
    "    with open(path.join(args.export_dir, \"topic-{}.values\".format(epoch)), 'w') as f:\n",
    "        f.write('\\n'.join(str_values))\n",
    "\n",
    "\n",
    "device = torch.device(args.device)\n",
    "\n",
    "conf = Conf(vocab_size=6906, hidden_size=65, latent_size=20)\n",
    "model = GSM(conf).to(device)\n",
    "\n",
    "task = TopicModelTask(\"topic model\", args)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "# load datasets\n",
    "train_loader, dev_loader, test_loader, vocab = load_kos_data(args, device)\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "writer = SummaryWriter(path.join(args.runs_dir, \"gsm\"))\n",
    "\n",
    "for e in range(1, args.epochs + 1):\n",
    "    stats = task.train(model, optim,  train_loader, device)\n",
    "\n",
    "    print(f\"{task.global_step}\\t | \" + stats.description('train_'))\n",
    "    write_summary(writer, stats, task.global_step, 'train')\n",
    "\n",
    "    if e % args.evaluate_every == 0:\n",
    "        stats = task.eval(model, dev_loader, device)\n",
    "        print(stats.description('-->eval_'))\n",
    "        write_summary(writer, stats, task.global_step, 'eval')\n",
    "\n",
    "        if stats.get_value(\"loss\") < best_loss:\n",
    "            print(\"new best model\")\n",
    "            best_loss = stats.get_value(\"loss\")\n",
    "            save_checkpoint(args, model)\n",
    "            topics = model.get_topics()\n",
    "            save_topics(args, vocab, topics, e, topk=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic\t0\n",
      "kerry edwards clark dean lieberman susa results gephardt sharpton kucinich\n",
      "topic\t1\n",
      "kerry dean edwards percent clark poll iowa bush primary lieberman\n",
      "topic\t2\n",
      "bloggers bush media john convention news kerry book people time\n",
      "topic\t3\n",
      "bush administration president health iraq war care white energy policy\n",
      "topic\t4\n",
      "party district campaign republican marriage race bush democrats ballot delay\n",
      "topic\t5\n",
      "media bloggers convention john time people ill aug blog news\n",
      "topic\t6\n",
      "november kerry chemical account exit qaqaa poll bush allegory kingelection\n",
      "topic\t7\n",
      "iraq sadr iraqi war shiite forces american military saddam fallujah\n",
      "topic\t8\n",
      "tax deficit bush income billion deficits jobs economy budget cuts\n",
      "topic\t9\n",
      "delay party republican republicans ryan ethics senate bloggers house committee\n",
      "topic\t10\n",
      "afscme dean iowa seiu percent mcentee primary poll specter hoeffel\n",
      "topic\t11\n",
      "subpoena thurlow sharks drake subpoenas protesters guild mosque shark myers\n",
      "topic\t12\n",
      "million district campaign party endorsement unions democratic money race democrats\n",
      "topic\t13\n",
      "november house republicans voting polls senate governor election electoral voter\n",
      "topic\t14\n",
      "media bloggers convention news ill john elections party blogs time\n",
      "topic\t15\n",
      "november electoral account republicans poll governor house experience chemical sunzoo\n",
      "topic\t16\n",
      "senate race state house republican campaign elections gop party democratic\n",
      "topic\t17\n",
      "november account electoral house senate governor republicans poll vote voting\n",
      "topic\t18\n",
      "bush intelligence administration iraq war weapons rice counterterrorism officials terrorism\n",
      "topic\t19\n",
      "petraeus november charge fallujah conway command iraqis iraq iraqi commander\n"
     ]
    }
   ],
   "source": [
    "def print_topics(vocab, topics, topk=10):\n",
    "    topics = topics.detach()\n",
    "    values, indices = torch.topk(topics, k=topk, dim=-1)\n",
    "\n",
    "    for idx, t in enumerate(indices):\n",
    "        print(f\"topic\\t{idx}\")\n",
    "        print(' '.join([vocab.get_word(i.item()) for i in t]))\n",
    "\n",
    "print_topics(vocab, topics)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see the 20 topic extracted are mostly related to politics, wars and election.\n",
    "Yet, the final perplexity obtained is significantly better w.r.t. the Dirichlet Mixture model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}