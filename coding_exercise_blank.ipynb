{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RASA](http://i.imgur.com/aImJD4o.png)\n",
    "\n",
    "\n",
    "## Instructions\n",
    "The point of this exercise is to give you the chance to show us what you know, can do, and how well you communicate what you find out. \n",
    "\n",
    "For this exercise to be useful we of course can't have solutions on the internet.\n",
    "This should go without saying but please don't distribute these questions in any form.\n",
    "\n",
    "When you feel you're ready please email your solution, with cell output, to the email address that sent this.\n",
    "If you like you can also include pdf/html exports in addition to the `.ipynb` file itself. **In your email you must include a statement that this is your original work and was completed only by you.**\n",
    "\n",
    "The exercise uses python, and the `numpy` and `matplotlib` libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: warm-up exercise.\n",
    "\n",
    "The function below draws a sample from an unnormalised discrete distribution.\n",
    "1. Explain how it works\n",
    "2. Write a vectorised version, which takes the number of samples to generate as an extra parameter, and returns a numpy array of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_discrete(b):\n",
    "    r = np.sum(b)*np.random.random()\n",
    "    a = b[0].copy()\n",
    "    i = 0\n",
    "    while a < r:\n",
    "        i += 1\n",
    "        a += b[i]\n",
    "    return i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def sample_vectoraized_discrete(b, n_samples):\n",
    "    # sample intervals\n",
    "    r = np.sum(b)*np.random.random([n_samples, 1])\n",
    "    # compute cumulative probability to compare with the sampled intervals\n",
    "    a = np.cumsum(b)\n",
    "    # return the index of the biggest cumulative value smaller than the sampled value.\n",
    "    # Note that this operation is done sample wise.\n",
    "    return np.sum(a < r, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of how it works, with probabilities `b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "b = np.array([0.1,0.2,0.3,0.4,0.3,0.5,0.2,0.03])\n",
    "n_samples = 10000\n",
    "bins = np.arange(b.size) - 0.5\n",
    "\n",
    "np.random.seed(0)\n",
    "samples = np.array([sample_discrete(b) for _ in range(n_samples)])\n",
    "\n",
    "# reset the seed to obtain equal sample for comparison\n",
    "np.random.seed(0)\n",
    "vectorized_samples = sample_vectoraized_discrete(b, n_samples)\n",
    "\n",
    "# make sure that code is correct\n",
    "assert np.all(samples == vectorized_samples)\n",
    "\n",
    "plt.hist(samples,bins, density=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Explanation 1\n",
    "\n",
    "Drawing samples from any probability distribution $P(X)$ is an important task since each samples can be used to\n",
    "approximate expectations of functions depending on $P(X)$.\n",
    "\n",
    "For any discrete distribution, given a probability mass function $p(x) = P(X=x)$; it's cumulative distribution function is:\n",
    "\n",
    "$$F(x_j) = \\sum_{i \\leq j} p(x_i)$$\n",
    "\n",
    "Now assume that $p(x)$ is a proper (normalized) probability mass function, then an easy way to sample from $p(x)$ is to:\n",
    "\n",
    " - lay out each possible outcome on a stick of length 1\n",
    " - sample a value $u$ from an uniform distribution\n",
    " - identify in which interval of the cumulative distribution function $u$ fall into and select and the associated $x_j$\n",
    "\n",
    "More formally, we can partition $F(x)$ in intervals:\n",
    "\n",
    "- $$ [\\Big(0, F(x_1) \\Big), \\Big(F(x_1), F(x_2) \\Big), ..., \\Big(F(x_k), 1 \\Big)]$$\n",
    "- Drawn $u \\sim Uniform(0, 1)$\n",
    "- Check in where interval $F(x_{i-1}) \\leq u \\leq F(x_i)$\n",
    "- Sample $F(x_i)$\n",
    "\n",
    "Such method can be easily extended to unnormalised discrete distribution, where instead to sampling $u$ from Uniform(0,1) we sample\n",
    "$u$ from $(0, F(X))$. Thus, we have:\n",
    "```python\n",
    "r = np.sum(b) * np.random().random()\n",
    "```\n",
    "Instead of:\n",
    "```python\n",
    "r = np.random().random()\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "This section is about unsupervised learning with text data.\n",
    "We'll be using a relatively small dataset, consisting of 3430 documents and a vocabulary of 6906 words drawn from the daily kos blog around 2004. \n",
    "\n",
    "You can download the data [here](https://s3-eu-west-1.amazonaws.com/lastmilecoding/exercise.tar.gz).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian mixture model\n",
    "\n",
    "We're going to model the documents as bags of words, with a bayesian mixture model.\n",
    "The documents are modeled using $K$ topics.\n",
    "The assignment of a document to a topic is modeled by the latent variable $z_d$.\n",
    "\n",
    "The topics are drawn from a categorical distribution with parameters $\\theta$, where $\\theta$ is drawn from a Dirichlet prior with parameter $\\alpha$.\n",
    "\n",
    "Each topic specifies a categorical distribution over words. The prior on each of these distributions is a Dirichlet with parameter $\\gamma$. \n",
    "\n",
    "\n",
    "The figure below shows this in a graphical model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Graphical Model](http://i.imgur.com/AAGnKZ7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, let's denote with $y_n$ the observations (i.e. the documents we see in the corpus).\n",
    "The conditional likelihood is:\n",
    "\n",
    "$$p(y_n|z_n \\negmedspace = \\negmedspace k,\\beta) = p(y_n|\\beta_k) = p(y_n|\\beta_{z_n})$$\n",
    "\n",
    "And we have a prior:\n",
    "\n",
    "$$p(\\beta_k)$$\n",
    "\n",
    "And a (latent) categorical assignment probability:\n",
    "\n",
    "$$p(z_n \\negmedspace= \\negmedspace k|\\theta) = \\theta_k$$\n",
    "\n",
    "which has a Dirichlet prior:\n",
    "\n",
    "$$p(\\theta|\\alpha) = Dir(\\alpha)$$\n",
    "\n",
    "Which gives our latent posterior:\n",
    "\n",
    "$$ p(z_n\\negmedspace=\\negmedspace k|y_n,\\theta,\\beta) \\quad \\propto p(z_n\\negmedspace =\\negmedspace k|\\theta)p(y_n|z_n \\negmedspace = \\negmedspace k,\\beta) \\quad \\propto \\theta_k p(y_n|\\beta_{z_n}) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gibbs Sampling\n",
    "We will explore this model by drawing from the posterior using MCMC, specifically Gibbs sampling.\n",
    "\n",
    "We will alternately sample the three types of variables, & iterate this procedure multiple times.\n",
    "\n",
    "First we'll sample the component parameters:\n",
    "\n",
    "$$p(\\beta_k|y,z) \\quad \\propto p(\\beta_k) \\prod_{n:z_n=k} p(y_n|\\beta_k) $$\n",
    "\n",
    "Then the latent allocations of documents to topics:\n",
    "$$ p(z_n \\negmedspace= \\negmedspace k|y_n,\\theta,\\beta) \\quad \\propto \\theta_k p(y_n|\\beta_{z_n})$$\n",
    "\n",
    "and then the mixing proportions:\n",
    "\n",
    "$$p(\\theta|z,\\alpha) = p(\\theta|\\alpha)p(z|\\theta) = \\mathrm{Dir}\\left(\\frac{c_k+\\alpha_k}{\\sum_j c_j + \\alpha_j}\\right)$$\n",
    "\n",
    "where $c_k$ are the counts for mixture component $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Collapsed Gibbs Sampler\n",
    "\n",
    "We marginalise over $\\theta$. (You do not need to derive this result).\n",
    "N.B. the notation $c_{-n}$ indicates all indices _except_ $n$. \n",
    "\n",
    "$$p(z_n \\negmedspace= \\negmedspace k|z_{-n},\\alpha) = \\frac{\\alpha + c_{-n,k}}{\\sum_j \\alpha + c_{-n,j}}$$\n",
    "\n",
    "which gives the _collapsed_ gibbs sampler for the latent assignments:\n",
    "\n",
    "$$p(z_n \\negmedspace= \\negmedspace k|y_n,z_{-n},\\beta,\\alpha) \\propto p(y_n|\\beta_k) \\frac{\\alpha + c_{-n,k}}{\\sum_j \\alpha + c_{-n,j}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "\n",
    "The documents have been split into two corpora, `A` and `B`.\n",
    "\n",
    "The array `words` is a list of all the words in both corpora.\n",
    "The matrices `A` and `B` are the train and test corpora, respectively. \n",
    "Each has 3 columns, there is one row for each unique word in each document.\n",
    "\n",
    "The first column is the document index, second is the word index (corresponding to `words`), and the third is the number of times that word appears in the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.load('data/mat_A.npy')\n",
    "B = np.load('data/mat_B.npy')\n",
    "words = np.load('data/words.npy')\n",
    "\n",
    "W = np.max(np.hstack((A[:,1],B[:,1]))) + 1   # number of unique words\n",
    "D = np.max(A[:,0]) + 1   # number of documents in A\n",
    "K = 20 # number of mixture components we will use\n",
    "\n",
    "alpha = 10  # parameter of the Dirichlet over mixture components\n",
    "gamma = 0.1 # parameter of the Dirichlet over words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below defines the following matrices:\n",
    " * `sd` : the mixture component assignment of each document\n",
    " * `swk` : K multinomial distributions over W unique words\n",
    " * `sk_docs` : the number of documents assigned to each mixture component\n",
    " * `sk_words` : the number of words assigned to mix component `k` accross all docs\n",
    " \n",
    "These are initialised by assigning each document to a mixture component at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    sd = np.floor(K*np.random.random((D,1))).astype(int)   \n",
    "    swk = np.zeros((W,K))              \n",
    "    sk_docs = np.zeros((K,1)) \n",
    "\n",
    "    for d in np.arange(D): \n",
    "        w = A[A[:,0]==d,1]      # unique words in doc d\n",
    "        c = A[A[:,0]==d,2]      # counts\n",
    "        k = sd[d]               # doc d is in mixture k\n",
    "        swk[w,k] = swk[w,k] + c # num times word w is assigned to mixture component k\n",
    "        sk_docs[k] = sk_docs[k] + 1\n",
    "\n",
    "    sk_words = np.sum(swk,axis=0).T\n",
    "    return sd, swk, sk_docs, sk_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code starts from this initial state & then performs a number of gibbs sampling sweeps. \n",
    "We will use the collapsed Gibbs sampler, which uses the trick of excluding the current document's counts before calculating the posterior and resampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gibbs sweep : 0\n",
      "gibbs sweep : 1\n",
      "gibbs sweep : 2\n",
      "gibbs sweep : 3\n",
      "gibbs sweep : 4\n",
      "gibbs sweep : 5\n",
      "gibbs sweep : 6\n",
      "gibbs sweep : 7\n",
      "gibbs sweep : 8\n",
      "gibbs sweep : 9\n"
     ]
    }
   ],
   "source": [
    "sd, swk, sk_docs, sk_words = init()\n",
    "# This makes a number of Gibbs sampling sweeps through all docs and words\n",
    "num_sweeps = 10\n",
    "for i_sweep in np.arange(num_sweeps): \n",
    "    print(\"gibbs sweep : {0}\".format(i_sweep))\n",
    "    for d in np.arange(D):\n",
    "        w = A[A[:,0]==d,1]      # unique words in doc d\n",
    "        c = A[A[:,0]==d,2]      # counts\n",
    "\n",
    "        # remove doc d's contributions from count tables\n",
    "        swk[w,sd[d]] = swk[w,sd[d]] - c \n",
    "        sk_docs[sd[d]] = sk_docs[sd[d]] - 1 \n",
    "        sk_words[sd[d]] = sk_words[sd[d]] - np.sum(c) \n",
    "        \n",
    "        # log probability of doc d under each mixture component\n",
    "        lb = np.zeros(K)    \n",
    "        for k in np.arange(K):\n",
    "            ll = np.dot(c,( np.log(swk[w,k]+gamma) - np.log(sk_words[k] + gamma*W) ))\n",
    "            lb[k] = np.log(sk_docs[k] + alpha) + ll\n",
    "\n",
    "        # assign doc d to a new component\n",
    "        b = np.exp(lb-np.max(lb))  \n",
    "        kk = sample_discrete(b)  \n",
    "\n",
    "        # add back doc d's contributions from count tables\n",
    "        swk[w,kk] = swk[w,kk] + c \n",
    "        sk_docs[kk] = sk_docs[kk] + 1 \n",
    "        sk_words[kk] = sk_words[kk] + np.sum(c)\n",
    "        sd[d] = kk"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2.1\n",
    "\n",
    "The code above is divided in:\n",
    "- a first part that define the model hyper-parameters. The most important are ``K, alpha`` and ``gamma``.\n",
    "- we randomly assign each document to a possible topic (```sd = np.floor(K*np.random.random((D,1))).astype(int)```)\n",
    "- based on the document assignment, it is possible to randomly initialize the counting matrixes\n",
    "\n",
    "\n",
    "Once all the needed data structure are randomly initialized, it is time to perform multiple steps of gibbs sampling to approximate the posterior distribution.\n",
    "Specifically, for each document ``d`` in my training set:\n",
    "- we exclude ``d`` from the current counting\n",
    "- we compute a new document-topic assignment based on the updated counting $$p(z_n \\negmedspace= \\negmedspace k|y_n,z_{-n},\\beta,\\alpha) \\propto p(y_n|\\beta_k) \\frac{\\alpha + c_{-n,k}}{\\sum_j \\alpha + c_{-n,j}}$$\n",
    "- we assign document ``d`` to a new topic based on the (log) probability previously computed\n",
    "- update the counting of our model according to the updated topic assignment\n",
    "\n",
    "\n",
    "I'm not really a fan of such model:\n",
    "- first of all, I would suggest a different parameter initialization. ``alpha = 10`` assume that a document might be associated to random topics.\n",
    "Topic models are usually adopted as an unsupervised method to do document classification/clustering over it's topic.\n",
    "Thus, we would like to have document associated to a relevant topic.\n",
    "``gamma = 0.1`` seems a common prior for topic-word distribution.\n",
    "- Moreover, your model assume that every word of a document come from the same topic (you sample the topic of each document only once, not for every word). Instead, a traditional LDA assume that every word in the document might be associated to a different topic coming from the same topic distribution.\n",
    "- As LDA, even this mixture model does not consider the words' order, thus has a limited understanding of the semantic of each document. A bi-gram model could highly prevent such issue.\n",
    "- Bayesian mixture models are highly sensitive to the noise present in each document. Thus, it require careful preprocessing such as: stopword removal, stemming.\n",
    "- On the one had, LDA is not easily applicable to social media content such as twitter, where short documents (tweets) does not present significant word co-occurence. On the other hand, this simpler model might overcome the sparisity issue associated to short text since the topic distribution is sampled once at corpus level\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2802277.4486418595\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import gammaln\n",
    "\n",
    "def log_ll(B, d, swk, sk_docs, sk_words, alpha, gamma, W, K):\n",
    "    w = B[B[:, 0] == d + 2000, 1]\n",
    "    c = B[B[:, 0] == d + 2000, 2]\n",
    "\n",
    "    t = np.zeros([K, W])\n",
    "    for k in np.arange(K):\n",
    "        th = (sk_docs[k] + alpha) / np.sum(sk_docs + alpha)\n",
    "        ph = (swk[:, k] + gamma) / (sk_words[k] + gamma * W)\n",
    "        t[k] = th * ph\n",
    "\n",
    "    p = c * np.log(t[:, w].sum(0))\n",
    "    return p.sum(0)\n",
    "\n",
    "\n",
    "print(log_ll(B, 0, swk, sk_docs, sk_words, alpha, gamma, W, K))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2.2\n",
    "\n",
    "The log-probability describe how probable is that the given document has been generated by the LDA model trained on the training set.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2698.6634819969004\n"
     ]
    }
   ],
   "source": [
    "def perplexity(B, swk, sk_docs, sk_words, alpha, gamma, W, K):\n",
    "    t = np.zeros([K, W])\n",
    "    for k in np.arange(K):\n",
    "        th = (sk_docs[k] + alpha) / np.sum(sk_docs + alpha)\n",
    "        ph = (swk[:, k] + gamma) / (sk_words[k] + gamma * W)\n",
    "        t[k] = th * ph\n",
    "\n",
    "    p = 0\n",
    "    for w in np.unique(B[:, 1]):\n",
    "        c = B[B[:, 1] == w, 2].sum()\n",
    "\n",
    "        p += c * np.log(t[:, w].sum(0))\n",
    "\n",
    "\n",
    "    p = p / B[:, 2].sum(0)\n",
    "\n",
    "    return np.exp(-p)\n",
    "\n",
    "print(perplexity(B,swk, sk_docs, sk_words, alpha, gamma, W, K))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2.3\n",
    "\n",
    "Perplexity define how much our model is surprised to see the test set.\n",
    "A smaller perplexity means the fitted model can better explain our test set.\n",
    "However, I don't really like perplexity to compare different language model because it is somehow dependant on the vocabulary size.\n",
    "Thus, the preprocessing and the token definition might make model comparison NOT straightforward.\n",
    "\n",
    "Yet, in some cases, perplexity is an efficient way to estimate the log-likelihood of a document in the test set.\n",
    "Specifically, perplexity is defined as:\n",
    "\n",
    "$$ p(W_{test} | \\mathbf{M}) = \\exp - \\Big( \\frac{ \\sum_{w \\in W_{test}} \\log p(w|\\mathbf{M}) } { \\sum_{w \\in W_{test}} n^w } \\Big) $$\n",
    "\n",
    "where $ \\mathbf{M}$ is the trained model, $W_{test}$ are the words in the test set and $n^w$ stand for the number of times word $w$ appears in the test set.\n",
    "\n",
    "Note that, I have computed $\\log p(w|\\mathbf{M})$ as:\n",
    "\n",
    "$$  \\log p(w|\\mathbf{M}) = n^w \\log \\big ( \\sum_{k=1}^K  \\phi_{k,w} \\theta_k \\big ) $$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3:\n",
    "Free form. Extend and/or analyse the above model, or use your favourite algorithm to find something interesting in this dataset. Show us what you can do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}