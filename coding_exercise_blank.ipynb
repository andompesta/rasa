{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RASA](http://i.imgur.com/aImJD4o.png)\n",
    "\n",
    "\n",
    "## Instructions\n",
    "The point of this exercise is to give you the chance to show us what you know, can do, and how well you communicate what you find out. \n",
    "\n",
    "For this exercise to be useful we of course can't have solutions on the internet.\n",
    "This should go without saying but please don't distribute these questions in any form.\n",
    "\n",
    "When you feel you're ready please email your solution, with cell output, to the email address that sent this.\n",
    "If you like you can also include pdf/html exports in addition to the `.ipynb` file itself. **In your email you must include a statement that this is your original work and was completed only by you.**\n",
    "\n",
    "The exercise uses python, and the `numpy` and `matplotlib` libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: warm-up exercise.\n",
    "\n",
    "The function below draws a sample from an unnormalised discrete distribution.\n",
    "1. Explain how it works\n",
    "2. Write a vectorised version, which takes the number of samples to generate as an extra parameter, and returns a numpy array of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_discrete(b):\n",
    "    r = np.sum(b)*np.random.random()\n",
    "    a = b[0].copy()\n",
    "    i = 0\n",
    "    while a < r:\n",
    "        i += 1\n",
    "        a += b[i]\n",
    "    return i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def sample_vectoraized_discrete(b, n_samples):\n",
    "    # sample intervals\n",
    "    r = np.sum(b)*np.random.random([n_samples, 1])\n",
    "    # compute cumulative probability to compare with the sampled intervals\n",
    "    a = np.cumsum(b)\n",
    "    # return the index of the biggest cumulative value smaller than the sampled value.\n",
    "    # Note that this operation is done sample wise.\n",
    "    return np.sum(a < r, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of how it works, with probabilities `b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOVklEQVR4nO3cf6jdd33H8efLG8O2rKVg72rJj90OwqQMO8sldXTUdVtLYmXpnxWnIEroaKdlyMz2h2Psn+6fsQnVLLQZyuzC0AbCGpsKbrih3XLjattUWy4xI5dYkqpTO8ESfe+P+4073J17z/cm9+Tc+/H5gHDv9/v9fM953xKePfnec76pKiRJ7XrDpAeQJI2XoZekxhl6SWqcoZekxhl6SWrcpkkPMMz1119fMzMzkx5DkjaMkydPvlpV08OO9Qp9kt3A3wBTwKNV9fCS4+8BPtptvgb8QVV9rTt2BvgB8GPgYlXNjnq+mZkZ5ubm+owmSQKS/Ndyx0aGPskU8AhwF7AAnEhytKpeHFj2TeAdVfXdJHuAg8BtA8fvrKpXL2t6SdIV6XONfhcwX1Wnq+p14DCwd3BBVX25qr7bbT4DbFvbMSVJl6tP6LcCZwe2F7p9y/kA8PmB7QKeTnIyyb7lTkqyL8lckrkLFy70GEuS1Eefa/QZsm/ofROS3Mli6H9zYPftVXUuyS8BX0jyjar60v97wKqDLF7yYXZ21vsySNIa6fOKfgHYPrC9DTi3dFGStwKPAnur6tuX9lfVue7reeAIi5eCJElXSZ/QnwB2JrkpyWbgPuDo4IIkO4AngPdW1csD+7ckuebS98DdwAtrNbwkabSRl26q6mKSB4HjLL698lBVnUpyf3f8APAx4E3AJ5LA/72N8gbgSLdvE/B4VT01lp9EkjRU1uNtimdnZ8v30UtSf0lOLvc5JW+BIEmNW5e3QJC0fs3sf3LSI6zKmYfvmfQIE+creklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMb1Cn2S3UleSjKfZP+Q4+9J8lz358tJbul7riRpvEaGPskU8AiwB7gZeHeSm5cs+ybwjqp6K/AXwMFVnCtJGqM+r+h3AfNVdbqqXgcOA3sHF1TVl6vqu93mM8C2vudKksZrU481W4GzA9sLwG0rrP8A8PnVnptkH7APYMeOHT3GkpY3s//JSY+wKmcevmfSI6hhfV7RZ8i+GrowuZPF0H90tedW1cGqmq2q2enp6R5jSZL66POKfgHYPrC9DTi3dFGStwKPAnuq6turOVeSND59XtGfAHYmuSnJZuA+4OjggiQ7gCeA91bVy6s5V5I0XiNf0VfVxSQPAseBKeBQVZ1Kcn93/ADwMeBNwCeSAFzsLsMMPXdMP4skaYg+l26oqmPAsSX7Dgx8/0Hgg33PlSRdPX4yVpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXG9Qp9kd5KXkswn2T/k+FuSfCXJj5J8ZMmxM0meT/Jskrm1GlyS1M+mUQuSTAGPAHcBC8CJJEer6sWBZd8BPgTcu8zD3FlVr17hrJKkyzAy9MAuYL6qTgMkOQzsBX4a+qo6D5xPcs9YppQaN7P/yUmPoIb1Cf1W4OzA9gJw2yqeo4CnkxTwt1V1cNiiJPuAfQA7duxYxcPrajFG0sbU5xp9huyrVTzH7VV1K7AHeCDJHcMWVdXBqpqtqtnp6elVPLwkaSV9Qr8AbB/Y3gac6/sEVXWu+3oeOMLipSBJ0lXSJ/QngJ1JbkqyGbgPONrnwZNsSXLNpe+Bu4EXLndYSdLqjbxGX1UXkzwIHAemgENVdSrJ/d3xA0neDMwB1wI/SfIQcDNwPXAkyaXneryqnhrLTyJJGqrPL2OpqmPAsSX7Dgx8/wqLl3SW+j5wy5UMKEm6Mn4yVpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIa1yv0SXYneSnJfJL9Q46/JclXkvwoyUdWc64kabw2jVqQZAp4BLgLWABOJDlaVS8OLPsO8CHg3ss492fWzP4nJz2CpJ8BI0MP7ALmq+o0QJLDwF7gp7GuqvPA+ST3rPZcSRqnjfSC6szDSxO6NvpcutkKnB3YXuj29dH73CT7kswlmbtw4ULPh5ckjdIn9Bmyr3o+fu9zq+pgVc1W1ez09HTPh5ckjdIn9AvA9oHtbcC5no9/JedKktZAn9CfAHYmuSnJZuA+4GjPx7+ScyVJa2DkL2Or6mKSB4HjwBRwqKpOJbm/O34gyZuBOeBa4CdJHgJurqrvDzt3TD+LJGmIPu+6oaqOAceW7Dsw8P0rLF6W6XWuJOnq8ZOxktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjesV+iS7k7yUZD7J/iHHk+Tj3fHnktw6cOxMkueTPJtkbi2HlySNtmnUgiRTwCPAXcACcCLJ0ap6cWDZHmBn9+c24JPd10vurKpX12zqFczsf/JqPI0kbRh9XtHvAuar6nRVvQ4cBvYuWbMX+HQtega4LsmNazyrJOky9An9VuDswPZCt6/vmgKeTnIyyb7lniTJviRzSeYuXLjQYyxJUh99Qp8h+2oVa26vqltZvLzzQJI7hj1JVR2sqtmqmp2enu4xliSpjz6hXwC2D2xvA871XVNVl76eB46weClIknSV9An9CWBnkpuSbAbuA44uWXMUeF/37pu3A9+rqm8l2ZLkGoAkW4C7gRfWcH5J0ggj33VTVReTPAgcB6aAQ1V1Ksn93fEDwDHgncA88EPg/d3pNwBHklx6rser6qk1/ykkScsaGXqAqjrGYswH9x0Y+L6AB4acdxq45QpnlCRdAT8ZK0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1LheoU+yO8lLSeaT7B9yPEk+3h1/Lsmtfc+VJI3XyNAnmQIeAfYANwPvTnLzkmV7gJ3dn33AJ1dxriRpjPq8ot8FzFfV6ap6HTgM7F2yZi/w6Vr0DHBdkht7nitJGqNNPdZsBc4ObC8At/VYs7XnuQAk2cfivwYAXkvyUo/ZrqbrgVcnPURPzjo+G2nejTQrbKx5xzJr/vKKTv/l5Q70CX2G7Kuea/qcu7iz6iBwsMc8E5FkrqpmJz1HH846Phtp3o00K2yseTfSrNAv9AvA9oHtbcC5nms29zhXkjRGfa7RnwB2JrkpyWbgPuDokjVHgfd17755O/C9qvpWz3MlSWM08hV9VV1M8iBwHJgCDlXVqST3d8cPAMeAdwLzwA+B96907lh+kvFbt5eVhnDW8dlI826kWWFjzbuRZiVVQy+ZS5Ia4SdjJalxhl6SGmfoR9hIt3BIcijJ+SQvTHqWUZJsT/LPSb6e5FSSD096ppUk+bkk/5Hka928fz7pmUZJMpXkP5P806RnGSXJmSTPJ3k2ydyk51lJkuuSfDbJN7q/v78x6ZlG8Rr9CrpbOLwM3MXiW0hPAO+uqhcnOtgyktwBvMbip5R/bdLzrKT75PSNVfXVJNcAJ4F71/F/2wBbquq1JG8E/g34cPdJ8HUpyR8Bs8C1VfWuSc+zkiRngNmqWvcfmEryKeBfq+rR7t2Ev1BV/z3hsVbkK/qVbahbOFTVl4DvTHqOPqrqW1X11e77HwBfZ/GT1OtSd3uP17rNN3Z/1u2rpCTbgHuARyc9S0uSXAvcATwGUFWvr/fIg6EfZblbO2gNJZkB3gb8+4RHWVF3KeRZ4Dzwhapaz/P+NfDHwE8mPEdfBTyd5GR3O5T16leAC8DfdZfFHk2yZdJDjWLoV9b7Fg66PEl+Efgc8FBVfX/S86ykqn5cVb/O4ie8dyVZl5fHkrwLOF9VJyc9yyrcXlW3snin2we6y5Dr0SbgVuCTVfU24H+Adf27OzD0o/S5/YMuU3et+3PAZ6rqiUnP01f3T/V/AXZPdpJl3Q78Xnfd+zDw20n+frIjrayqznVfzwNHWLxsuh4tAAsD/5r7LIvhX9cM/cq8hcOYdL/cfAz4elX91aTnGSXJdJLruu9/Hvhd4BsTHWoZVfUnVbWtqmZY/Dv7xar6/QmPtawkW7pfyNNdBrkbWJfvHKuqV4CzSX612/U7wLp8A8GgPjc1+5m10W7hkOQfgN8Crk+yAPxZVT022amWdTvwXuD57ro3wJ9W1bHJjbSiG4FPde/EegPwj1W17t+2uEHcABxZ/H8/m4DHq+qpyY60oj8EPtO9+DtNd8uX9cy3V0pS47x0I0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mN+18lMRosEy/jhQAAAABJRU5ErkJggg==\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "b = np.array([0.1,0.2,0.3,0.4,0.3,0.5,0.2,0.03])\n",
    "n_samples = 10000\n",
    "bins = np.arange(b.size) - 0.5\n",
    "\n",
    "np.random.seed(0)\n",
    "samples = np.array([sample_discrete(b) for _ in range(n_samples)])\n",
    "\n",
    "# reset the seed to obtain equal sample for comparison\n",
    "np.random.seed(0)\n",
    "vectorized_samples = sample_vectoraized_discrete(b, n_samples)\n",
    "\n",
    "# make sure that code is correct\n",
    "assert np.all(samples == vectorized_samples)\n",
    "\n",
    "plt.hist(samples,bins, density=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Explanation 1\n",
    "\n",
    "Drawing samples from any probability distribution $P(X)$ is an important task since each samples can be used to\n",
    "approximate expectations of functions depending on $P(X)$.\n",
    "\n",
    "For any discrete distribution, given a probability mass function $p(x) = P(X=x)$; it's cumulative distribution function is:\n",
    "\n",
    "$$F(x_j) = \\sum_{i \\leq j} p(x_i)$$\n",
    "\n",
    "Now assume that $p(x)$ is a proper (normalized) probability mass function, then an easy way to sample from $p(x)$ is to:\n",
    "\n",
    " - lay out each possible outcome on a stick of length 1\n",
    " - sample a value $u$ from an uniform distribution\n",
    " - identify in which interval of the cumulative distribution function $u$ fall into and select and the associated $x_j$\n",
    "\n",
    "More formally, we can partition $F(x)$ in intervals:\n",
    "\n",
    "- $$ [\\Big(0, F(x_1) \\Big), \\Big(F(x_1), F(x_2) \\Big), ..., \\Big(F(x_k), 1 \\Big)]$$\n",
    "- Drawn $u \\sim Uniform(0, 1)$\n",
    "- Check in where interval $F(x_{i-1}) \\leq u \\leq F(x_i)$\n",
    "- Sample $F(x_i)$\n",
    "\n",
    "Such method can be easily extended to unnormalised discrete distribution, where instead to sampling $u$ from Uniform(0,1) we sample\n",
    "$u$ from $(0, F(X))$. Thus, we have:\n",
    "```python\n",
    "r = np.sum(b) * np.random().random()\n",
    "```\n",
    "Instead of:\n",
    "```python\n",
    "r = np.random().random()\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "This section is about unsupervised learning with text data.\n",
    "We'll be using a relatively small dataset, consisting of 3430 documents and a vocabulary of 6906 words drawn from the daily kos blog around 2004. \n",
    "\n",
    "You can download the data [here](https://s3-eu-west-1.amazonaws.com/lastmilecoding/exercise.tar.gz).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian mixture model\n",
    "\n",
    "We're going to model the documents as bags of words, with a bayesian mixture model.\n",
    "The documents are modeled using $K$ topics.\n",
    "The assignment of a document to a topic is modeled by the latent variable $z_d$.\n",
    "\n",
    "The topics are drawn from a categorical distribution with parameters $\\theta$, where $\\theta$ is drawn from a Dirichlet prior with parameter $\\alpha$.\n",
    "\n",
    "Each topic specifies a categorical distribution over words. The prior on each of these distributions is a Dirichlet with parameter $\\gamma$. \n",
    "\n",
    "\n",
    "The figure below shows this in a graphical model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Graphical Model](http://i.imgur.com/AAGnKZ7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, let's denote with $y_n$ the observations (i.e. the documents we see in the corpus).\n",
    "The conditional likelihood is:\n",
    "\n",
    "$$p(y_n|z_n \\negmedspace = \\negmedspace k,\\beta) = p(y_n|\\beta_k) = p(y_n|\\beta_{z_n})$$\n",
    "\n",
    "And we have a prior:\n",
    "\n",
    "$$p(\\beta_k)$$\n",
    "\n",
    "And a (latent) categorical assignment probability:\n",
    "\n",
    "$$p(z_n \\negmedspace= \\negmedspace k|\\theta) = \\theta_k$$\n",
    "\n",
    "which has a Dirichlet prior:\n",
    "\n",
    "$$p(\\theta|\\alpha) = Dir(\\alpha)$$\n",
    "\n",
    "Which gives our latent posterior:\n",
    "\n",
    "$$ p(z_n\\negmedspace=\\negmedspace k|y_n,\\theta,\\beta) \\quad \\propto p(z_n\\negmedspace =\\negmedspace k|\\theta)p(y_n|z_n \\negmedspace = \\negmedspace k,\\beta) \\quad \\propto \\theta_k p(y_n|\\beta_{z_n}) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gibbs Sampling\n",
    "We will explore this model by drawing from the posterior using MCMC, specifically Gibbs sampling.\n",
    "\n",
    "We will alternately sample the three types of variables, & iterate this procedure multiple times.\n",
    "\n",
    "First we'll sample the component parameters:\n",
    "\n",
    "$$p(\\beta_k|y,z) \\quad \\propto p(\\beta_k) \\prod_{n:z_n=k} p(y_n|\\beta_k) $$\n",
    "\n",
    "Then the latent allocations of documents to topics:\n",
    "$$ p(z_n \\negmedspace= \\negmedspace k|y_n,\\theta,\\beta) \\quad \\propto \\theta_k p(y_n|\\beta_{z_n})$$\n",
    "\n",
    "and then the mixing proportions:\n",
    "\n",
    "$$p(\\theta|z,\\alpha) = p(\\theta|\\alpha)p(z|\\theta) = \\mathrm{Dir}\\left(\\frac{c_k+\\alpha_k}{\\sum_j c_j + \\alpha_j}\\right)$$\n",
    "\n",
    "where $c_k$ are the counts for mixture component $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Collapsed Gibbs Sampler\n",
    "\n",
    "We marginalise over $\\theta$. (You do not need to derive this result).\n",
    "N.B. the notation $c_{-n}$ indicates all indices _except_ $n$. \n",
    "\n",
    "$$p(z_n \\negmedspace= \\negmedspace k|z_{-n},\\alpha) = \\frac{\\alpha + c_{-n,k}}{\\sum_j \\alpha + c_{-n,j}}$$\n",
    "\n",
    "which gives the _collapsed_ gibbs sampler for the latent assignments:\n",
    "\n",
    "$$p(z_n \\negmedspace= \\negmedspace k|y_n,z_{-n},\\beta,\\alpha) \\propto p(y_n|\\beta_k) \\frac{\\alpha + c_{-n,k}}{\\sum_j \\alpha + c_{-n,j}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "\n",
    "The documents have been split into two corpora, `A` and `B`.\n",
    "\n",
    "The array `words` is a list of all the words in both corpora.\n",
    "The matrices `A` and `B` are the train and test corpora, respectively. \n",
    "Each has 3 columns, there is one row for each unique word in each document.\n",
    "\n",
    "The first column is the document index, second is the word index (corresponding to `words`), and the third is the number of times that word appears in the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.load('data/mat_A.npy')\n",
    "B = np.load('data/mat_B.npy')\n",
    "words = np.load('data/words.npy')\n",
    "\n",
    "W = np.max(np.hstack((A[:,1],B[:,1]))) + 1   # number of unique words\n",
    "D = np.max(A[:,0]) + 1   # number of documents in A\n",
    "K = 20 # number of mixture components we will use\n",
    "\n",
    "alpha = 10  # parameter of the Dirichlet over mixture components\n",
    "gamma = 0.1 # parameter of the Dirichlet over words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below defines the following matrices:\n",
    " * `sd` : the mixture component assignment of each document\n",
    " * `swk` : K multinomial distributions over W unique words\n",
    " * `sk_docs` : the number of documents assigned to each mixture component\n",
    " * `sk_words` : the number of words assigned to mix component `k` accross all docs\n",
    " \n",
    "These are initialised by assigning each document to a mixture component at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    sd = np.floor(K*np.random.random((D,1))).astype(int)   \n",
    "    swk = np.zeros((W,K))              \n",
    "    sk_docs = np.zeros((K,1)) \n",
    "\n",
    "    for d in np.arange(D): \n",
    "        w = A[A[:,0]==d,1]      # unique words in doc d\n",
    "        c = A[A[:,0]==d,2]      # counts\n",
    "        k = sd[d]               # doc d is in mixture k\n",
    "        swk[w,k] = swk[w,k] + c # num times word w is assigned to mixture component k\n",
    "        sk_docs[k] = sk_docs[k] + 1\n",
    "\n",
    "    sk_words = np.sum(swk,axis=0).T\n",
    "    return sd, swk, sk_docs, sk_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code starts from this initial state & then performs a number of gibbs sampling sweeps. \n",
    "We will use the collapsed Gibbs sampler, which uses the trick of excluding the current document's counts before calculating the posterior and resampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gibbs sweep : 0\n",
      "gibbs sweep : 1\n",
      "gibbs sweep : 2\n",
      "gibbs sweep : 3\n",
      "gibbs sweep : 4\n",
      "gibbs sweep : 5\n",
      "gibbs sweep : 6\n",
      "gibbs sweep : 7\n",
      "gibbs sweep : 8\n",
      "gibbs sweep : 9\n"
     ]
    }
   ],
   "source": [
    "sd, swk, sk_docs, sk_words = init()\n",
    "# This makes a number of Gibbs sampling sweeps through all docs and words\n",
    "num_sweeps = 10\n",
    "for i_sweep in np.arange(num_sweeps): \n",
    "    print(\"gibbs sweep : {0}\".format(i_sweep))\n",
    "    for d in np.arange(D):\n",
    "        w = A[A[:,0]==d,1]      # unique words in doc d\n",
    "        c = A[A[:,0]==d,2]      # counts\n",
    "\n",
    "        # remove doc d's contributions from count tables\n",
    "        swk[w,sd[d]] = swk[w,sd[d]] - c \n",
    "        sk_docs[sd[d]] = sk_docs[sd[d]] - 1 \n",
    "        sk_words[sd[d]] = sk_words[sd[d]] - np.sum(c) \n",
    "        \n",
    "        # log probability of doc d under each mixture component\n",
    "        lb = np.zeros(K)    \n",
    "        for k in np.arange(K):\n",
    "            ll = np.dot(c,( np.log(swk[w,k]+gamma) - np.log(sk_words[k] + gamma*W) ))\n",
    "            lb[k] = np.log(sk_docs[k] + alpha) + ll\n",
    "\n",
    "        # assign doc d to a new component\n",
    "        b = np.exp(lb-np.max(lb))  \n",
    "        kk = sample_discrete(b)  \n",
    "\n",
    "        # add back doc d's contributions from count tables\n",
    "        swk[w,kk] = swk[w,kk] + c \n",
    "        sk_docs[kk] = sk_docs[kk] + 1 \n",
    "        sk_words[kk] = sk_words[kk] + np.sum(c)\n",
    "        sd[d] = kk"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2.1\n",
    "\n",
    "The code above is divided in:\n",
    "- a first part that define the model hyper-parameters. The most important are ``K, alpha`` and ``gamma``.\n",
    "- we randomly assign each document to a possible topic (```sd = np.floor(K*np.random.random((D,1))).astype(int)```)\n",
    "- based on the document assignment, it is possible to randomly initialize the counting matrixes\n",
    "\n",
    "\n",
    "Once all the needed data structure are randomly initialized, it is time to perform multiple steps of gibbs sampling to approximate the posterior distribution.\n",
    "Specifically, for each document ``d`` in my training set:\n",
    "- we exclude ``d`` from the current counting\n",
    "- we compute a new document-topic assignment based on the updated counting $$p(z_n \\negmedspace= \\negmedspace k|y_n,z_{-n},\\beta,\\alpha) \\propto p(y_n|\\beta_k) \\frac{\\alpha + c_{-n,k}}{\\sum_j \\alpha + c_{-n,j}}$$\n",
    "- we assign document ``d`` to a new topic based on the (log) probability previously computed\n",
    "- update the counting of our model according to the updated topic assignment\n",
    "\n",
    "\n",
    "I'm not really a fan of such model:\n",
    "- first of all, I would suggest a different parameter initialization. ``alpha = 10`` assume that a document might be associated to random topics.\n",
    "Topic models are usually adopted as an unsupervised method to do document classification/clustering over it's topic.\n",
    "Thus, we would like to have document associated to a relevant topic.\n",
    "``gamma = 0.1`` seems a common prior for topic-word distribution.\n",
    "- Moreover, your model assume that every word of a document come from the same topic (you sample the topic of each document only once, not for every word). Instead, a traditional LDA assume that every word in the document might be associated to a different topic coming from the same topic distribution.\n",
    "- As LDA, even this mixture model does not consider the words' order, thus has a limited understanding of the semantic of each document. A bi-gram model could highly prevent such issue.\n",
    "- Bayesian mixture models are highly sensitive to the noise present in each document. Thus, it require careful preprocessing such as: stopword removal, stemming.\n",
    "- On the one had, LDA is not easily applicable to social media content such as twitter, where short documents (tweets) does not present significant word co-occurence. On the other hand, this simpler model might overcome the sparisity issue associated to short text since the topic distribution is sampled once at corpus level\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2802277.4486418595\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import gammaln\n",
    "\n",
    "def log_ll(B, d, swk, sk_docs, sk_words, alpha, gamma, W, K):\n",
    "    w = B[B[:, 0] == d + 2000, 1]\n",
    "    c = B[B[:, 0] == d + 2000, 2]\n",
    "\n",
    "    t = np.zeros([K, W])\n",
    "    for k in np.arange(K):\n",
    "        th = (sk_docs[k] + alpha) / np.sum(sk_docs + alpha)\n",
    "        ph = (swk[:, k] + gamma) / (sk_words[k] + gamma * W)\n",
    "        t[k] = th * ph\n",
    "\n",
    "    p = c * np.log(t[:, w].sum(0))\n",
    "    return p.sum(0)\n",
    "\n",
    "\n",
    "print(log_ll(B, 0, swk, sk_docs, sk_words, alpha, gamma, W, K))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2.2\n",
    "\n",
    "The log-probability describe how probable is that the given document has been generated by the LDA model trained on the training set.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2698.6634819969004\n"
     ]
    }
   ],
   "source": [
    "def perplexity(B, swk, sk_docs, sk_words, alpha, gamma, W, K):\n",
    "    t = np.zeros([K, W])\n",
    "    for k in np.arange(K):\n",
    "        th = (sk_docs[k] + alpha) / np.sum(sk_docs + alpha)\n",
    "        ph = (swk[:, k] + gamma) / (sk_words[k] + gamma * W)\n",
    "        t[k] = th * ph\n",
    "\n",
    "    p = 0\n",
    "    for w in np.unique(B[:, 1]):\n",
    "        c = B[B[:, 1] == w, 2].sum()\n",
    "\n",
    "        p += c * np.log(t[:, w].sum(0))\n",
    "\n",
    "\n",
    "    p = p / B[:, 2].sum(0)\n",
    "\n",
    "    return np.exp(-p)\n",
    "\n",
    "print(perplexity(B,swk, sk_docs, sk_words, alpha, gamma, W, K))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2.3\n",
    "\n",
    "Perplexity define how much our model is surprised to see the test set.\n",
    "A smaller perplexity means the fitted model can better explain our test set.\n",
    "However, I don't really like perplexity to compare different language model because it is somehow dependant on the vocabulary size.\n",
    "Thus, the preprocessing and the token definition might make model comparison NOT straightforward.\n",
    "\n",
    "Yet, in some cases, perplexity is an efficient way to estimate the log-likelihood of a document in the test set.\n",
    "Specifically, perplexity is defined as:\n",
    "\n",
    "$$ p(W_{test} | \\mathbf{M}) = \\exp - \\Big( \\frac{ \\sum_{w \\in W_{test}} \\log p(w|\\mathbf{M}) } { \\sum_{w \\in W_{test}} n^w } \\Big) $$\n",
    "\n",
    "where $ \\mathbf{M}$ is the trained model, $W_{test}$ are the words in the test set and $n^w$ stand for the number of times word $w$ appears in the test set.\n",
    "\n",
    "Note that, I have computed $\\log p(w|\\mathbf{M})$ as:\n",
    "\n",
    "$$  \\log p(w|\\mathbf{M}) = n^w \\log \\big ( \\sum_{k=1}^K  \\phi_{k,w} \\theta_k \\big ) $$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3:\n",
    "Free form. Extend and/or analyse the above model, or use your favourite algorithm to find something interesting in this dataset. Show us what you can do!\n",
    "\n",
    "\n",
    "Here you can find a minimal implementation of the [GSC](https://arxiv.org/pdf/1706.00359.pdf) model: a topic language model based on variational inference.\n",
    "It is similar to a VAE, but the extractor network $q(\\theta|d)$ is used to extract the topic of a document (as softmax of the sampled latent space).\n",
    "Whereas the generator network $p(x|\\theta)$ is a simple MLP that generate the bag-of-word representation of a document from a single topic distribution $\\theta$.\n",
    "\n",
    "P.S. the code is inspired by the [NDM](https://github.com/YongfeiYan/Neural-Document-Modeling) library.\n",
    "Finally, you need to manyally create the following directories before run the model:\n",
    " - export_dir: save the trained model\n",
    " - runs: contains the tensorboard logs\n",
    " - data: contains the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\t | train_ppx 6310.26660 | train_ppx_doc 6337.61719 | train_loss 1194.69250 | train_loss_rec 1188.80859 | train_kld 0.73617 | train_penalty 0.10295 | train_penalty_mean 0.14634 | train_penalty_var 0.04339\n",
      "40\t | train_ppx 4796.49756 | train_ppx_doc 4928.51172 | train_loss 1157.32446 | train_loss_rec 1146.03796 | train_kld 6.21694 | train_penalty 0.10139 | train_penalty_mean 0.14493 | train_penalty_var 0.04354\n",
      "60\t | train_ppx 3672.08960 | train_ppx_doc 3779.74365 | train_loss 1120.95801 | train_loss_rec 1107.54602 | train_kld 8.39364 | train_penalty 0.10037 | train_penalty_mean 0.14406 | train_penalty_var 0.04369\n",
      "80\t | train_ppx 3199.26929 | train_ppx_doc 3282.90869 | train_loss 1102.12378 | train_loss_rec 1091.46509 | train_kld 5.73540 | train_penalty 0.09846 | train_penalty_mean 0.14241 | train_penalty_var 0.04395\n",
      "100\t | train_ppx 2979.72729 | train_ppx_doc 3013.07788 | train_loss 1092.35657 | train_loss_rec 1082.89148 | train_kld 4.64428 | train_penalty 0.09641 | train_penalty_mean 0.14064 | train_penalty_var 0.04422\n",
      "-->eval_ppx 2825.25098 | -->eval_ppx_doc 2897.21631 | -->eval_loss 1092.89880 | -->eval_loss_rec 1083.77759 | -->eval_kld 4.35034 | -->eval_penalty 0.09542 | -->eval_penalty_mean 0.13979 | -->eval_penalty_var 0.04437\n",
      "new best model\n",
      "120\t | train_ppx 2892.01953 | train_ppx_doc 2916.62329 | train_loss 1088.19824 | train_loss_rec 1079.32153 | train_kld 4.15269 | train_penalty 0.09448 | train_penalty_mean 0.13900 | train_penalty_var 0.04452\n",
      "140\t | train_ppx 2783.24316 | train_ppx_doc 2813.29443 | train_loss 1082.90930 | train_loss_rec 1074.12842 | train_kld 4.13386 | train_penalty 0.09294 | train_penalty_mean 0.13773 | train_penalty_var 0.04479\n",
      "160\t | train_ppx 2668.83838 | train_ppx_doc 2700.16895 | train_loss 1077.12915 | train_loss_rec 1068.62451 | train_kld 3.93138 | train_penalty 0.09146 | train_penalty_mean 0.13648 | train_penalty_var 0.04502\n",
      "180\t | train_ppx 2609.27563 | train_ppx_doc 2644.93335 | train_loss 1073.98303 | train_loss_rec 1065.67822 | train_kld 3.80912 | train_penalty 0.08991 | train_penalty_mean 0.13517 | train_penalty_var 0.04526\n",
      "200\t | train_ppx 2586.93774 | train_ppx_doc 2600.49658 | train_loss 1072.74158 | train_loss_rec 1064.76465 | train_kld 3.55400 | train_penalty 0.08846 | train_penalty_mean 0.13393 | train_penalty_var 0.04547\n",
      "-->eval_ppx 2484.48950 | -->eval_ppx_doc 2548.48315 | -->eval_loss 1074.90906 | -->eval_loss_rec 1067.30420 | -->eval_kld 3.22336 | -->eval_penalty 0.08763 | -->eval_penalty_mean 0.13322 | -->eval_penalty_var 0.04559\n",
      "new best model\n",
      "220\t | train_ppx 2520.38672 | train_ppx_doc 2548.38721 | train_loss 1069.12061 | train_loss_rec 1061.19238 | train_kld 3.58279 | train_penalty 0.08691 | train_penalty_mean 0.13259 | train_penalty_var 0.04568\n",
      "240\t | train_ppx 2435.78564 | train_ppx_doc 2468.10132 | train_loss 1064.39832 | train_loss_rec 1056.94189 | train_kld 3.19166 | train_penalty 0.08530 | train_penalty_mean 0.13120 | train_penalty_var 0.04591\n",
      "260\t | train_ppx 2441.75830 | train_ppx_doc 2474.31958 | train_loss 1064.66064 | train_loss_rec 1057.18884 | train_kld 3.27781 | train_penalty 0.08388 | train_penalty_mean 0.12998 | train_penalty_var 0.04610\n",
      "280\t | train_ppx 2454.26709 | train_ppx_doc 2468.84399 | train_loss 1065.29321 | train_loss_rec 1057.97864 | train_kld 3.18257 | train_penalty 0.08264 | train_penalty_mean 0.12892 | train_penalty_var 0.04628\n",
      "300\t | train_ppx 2383.45093 | train_ppx_doc 2419.46045 | train_loss 1061.23877 | train_loss_rec 1054.07239 | train_kld 3.10840 | train_penalty 0.08116 | train_penalty_mean 0.12765 | train_penalty_var 0.04650\n",
      "-->eval_ppx 2375.69336 | -->eval_ppx_doc 2429.04932 | -->eval_loss 1068.40747 | -->eval_loss_rec 1061.52905 | -->eval_kld 2.86694 | -->eval_penalty 0.08023 | -->eval_penalty_mean 0.12684 | -->eval_penalty_var 0.04661\n",
      "new best model\n",
      "320\t | train_ppx 2377.53711 | train_ppx_doc 2404.77368 | train_loss 1060.81396 | train_loss_rec 1053.62256 | train_kld 3.22073 | train_penalty 0.07942 | train_penalty_mean 0.12613 | train_penalty_var 0.04671\n",
      "340\t | train_ppx 2372.22485 | train_ppx_doc 2388.08228 | train_loss 1060.41748 | train_loss_rec 1053.51013 | train_kld 3.02876 | train_penalty 0.07757 | train_penalty_mean 0.12444 | train_penalty_var 0.04687\n",
      "360\t | train_ppx 2354.45435 | train_ppx_doc 2373.51709 | train_loss 1059.31323 | train_loss_rec 1052.66211 | train_kld 2.85452 | train_penalty 0.07593 | train_penalty_mean 0.12296 | train_penalty_var 0.04702\n",
      "380\t | train_ppx 2335.17310 | train_ppx_doc 2355.77173 | train_loss 1058.09241 | train_loss_rec 1051.52771 | train_kld 2.87094 | train_penalty 0.07387 | train_penalty_mean 0.12105 | train_penalty_var 0.04718\n",
      "400\t | train_ppx 2300.61987 | train_ppx_doc 2325.90161 | train_loss 1055.96375 | train_loss_rec 1049.31934 | train_kld 3.05295 | train_penalty 0.07183 | train_penalty_mean 0.11917 | train_penalty_var 0.04734\n",
      "-->eval_ppx 2321.27905 | -->eval_ppx_doc 2360.64966 | -->eval_loss 1064.76245 | -->eval_loss_rec 1058.48450 | -->eval_kld 2.73878 | -->eval_penalty 0.07079 | -->eval_penalty_mean 0.11820 | -->eval_penalty_var 0.04741\n",
      "new best model\n",
      "420\t | train_ppx 2297.09668 | train_ppx_doc 2318.07251 | train_loss 1055.65125 | train_loss_rec 1049.06287 | train_kld 3.10120 | train_penalty 0.06974 | train_penalty_mean 0.11722 | train_penalty_var 0.04748\n",
      "440\t | train_ppx 2268.32642 | train_ppx_doc 2298.27344 | train_loss 1053.83020 | train_loss_rec 1047.18213 | train_kld 3.26832 | train_penalty 0.06760 | train_penalty_mean 0.11523 | train_penalty_var 0.04763\n",
      "460\t | train_ppx 2261.36060 | train_ppx_doc 2278.83276 | train_loss 1053.30286 | train_loss_rec 1046.78076 | train_kld 3.25143 | train_penalty 0.06541 | train_penalty_mean 0.11321 | train_penalty_var 0.04780\n",
      "480\t | train_ppx 2271.39380 | train_ppx_doc 2278.26343 | train_loss 1053.81055 | train_loss_rec 1047.18005 | train_kld 3.45405 | train_penalty 0.06353 | train_penalty_mean 0.11147 | train_penalty_var 0.04794\n",
      "500\t | train_ppx 2237.55396 | train_ppx_doc 2255.77222 | train_loss 1051.69324 | train_loss_rec 1045.31311 | train_kld 3.28043 | train_penalty 0.06199 | train_penalty_mean 0.11005 | train_penalty_var 0.04805\n",
      "-->eval_ppx 2257.83081 | -->eval_ppx_doc 2298.99683 | -->eval_loss 1060.48157 | -->eval_loss_rec 1054.18542 | -->eval_kld 3.24294 | -->eval_penalty 0.06106 | -->eval_penalty_mean 0.10918 | -->eval_penalty_var 0.04812\n",
      "new best model\n",
      "520\t | train_ppx 2222.24683 | train_ppx_doc 2238.11963 | train_loss 1050.67676 | train_loss_rec 1044.03357 | train_kld 3.62651 | train_penalty 0.06033 | train_penalty_mean 0.10850 | train_penalty_var 0.04817\n",
      "540\t | train_ppx 2219.22778 | train_ppx_doc 2243.38062 | train_loss 1050.41699 | train_loss_rec 1043.82202 | train_kld 3.65361 | train_penalty 0.05883 | train_penalty_mean 0.10708 | train_penalty_var 0.04825\n",
      "560\t | train_ppx 2209.26611 | train_ppx_doc 2230.76685 | train_loss 1049.74670 | train_loss_rec 1043.12683 | train_kld 3.73702 | train_penalty 0.05766 | train_penalty_mean 0.10599 | train_penalty_var 0.04833\n",
      "580\t | train_ppx 2168.38794 | train_ppx_doc 2185.86182 | train_loss 1047.13501 | train_loss_rec 1040.71045 | train_kld 3.61431 | train_penalty 0.05620 | train_penalty_mean 0.10463 | train_penalty_var 0.04842\n",
      "600\t | train_ppx 2193.88770 | train_ppx_doc 2210.26074 | train_loss 1048.66138 | train_loss_rec 1042.21924 | train_kld 3.69496 | train_penalty 0.05494 | train_penalty_mean 0.10344 | train_penalty_var 0.04850\n",
      "-->eval_ppx 2219.59619 | -->eval_ppx_doc 2254.49365 | -->eval_loss 1057.80957 | -->eval_loss_rec 1051.48206 | -->eval_kld 3.60751 | -->eval_penalty 0.05440 | -->eval_penalty_mean 0.10296 | -->eval_penalty_var 0.04855\n",
      "new best model\n",
      "620\t | train_ppx 2171.66919 | train_ppx_doc 2187.66260 | train_loss 1047.23206 | train_loss_rec 1040.63989 | train_kld 3.89032 | train_penalty 0.05403 | train_penalty_mean 0.10262 | train_penalty_var 0.04859\n",
      "640\t | train_ppx 2130.73193 | train_ppx_doc 2152.04395 | train_loss 1044.59473 | train_loss_rec 1038.04272 | train_kld 3.90064 | train_penalty 0.05303 | train_penalty_mean 0.10171 | train_penalty_var 0.04868\n",
      "660\t | train_ppx 2135.51147 | train_ppx_doc 2160.55640 | train_loss 1044.83740 | train_loss_rec 1038.46045 | train_kld 3.78744 | train_penalty 0.05179 | train_penalty_mean 0.10056 | train_penalty_var 0.04877\n",
      "680\t | train_ppx 2134.35913 | train_ppx_doc 2154.57202 | train_loss 1044.70740 | train_loss_rec 1038.27515 | train_kld 3.89908 | train_penalty 0.05066 | train_penalty_mean 0.09951 | train_penalty_var 0.04885\n",
      "700\t | train_ppx 2124.57764 | train_ppx_doc 2150.13110 | train_loss 1044.02136 | train_loss_rec 1037.64587 | train_kld 3.90390 | train_penalty 0.04943 | train_penalty_mean 0.09835 | train_penalty_var 0.04892\n",
      "-->eval_ppx 2192.89307 | -->eval_ppx_doc 2226.65942 | -->eval_loss 1055.87036 | -->eval_loss_rec 1049.53528 | -->eval_kld 3.89688 | -->eval_penalty 0.04876 | -->eval_penalty_mean 0.09771 | -->eval_penalty_var 0.04894\n",
      "new best model\n",
      "720\t | train_ppx 2135.68652 | train_ppx_doc 2150.37915 | train_loss 1044.67212 | train_loss_rec 1038.00195 | train_kld 4.25686 | train_penalty 0.04827 | train_penalty_mean 0.09723 | train_penalty_var 0.04896\n",
      "740\t | train_ppx 2100.62988 | train_ppx_doc 2121.90576 | train_loss 1042.37280 | train_loss_rec 1036.13220 | train_kld 3.87664 | train_penalty 0.04727 | train_penalty_mean 0.09630 | train_penalty_var 0.04902\n",
      "760\t | train_ppx 2100.78296 | train_ppx_doc 2118.48877 | train_loss 1042.33423 | train_loss_rec 1035.92456 | train_kld 4.09431 | train_penalty 0.04631 | train_penalty_mean 0.09538 | train_penalty_var 0.04908\n",
      "780\t | train_ppx 2102.43164 | train_ppx_doc 2128.13647 | train_loss 1042.38623 | train_loss_rec 1035.96375 | train_kld 4.16177 | train_penalty 0.04522 | train_penalty_mean 0.09433 | train_penalty_var 0.04912\n",
      "800\t | train_ppx 2077.73047 | train_ppx_doc 2107.42603 | train_loss 1040.73108 | train_loss_rec 1034.52283 | train_kld 3.99611 | train_penalty 0.04425 | train_penalty_mean 0.09340 | train_penalty_var 0.04916\n",
      "-->eval_ppx 2178.53003 | -->eval_ppx_doc 2207.21167 | -->eval_loss 1054.72205 | -->eval_loss_rec 1048.69312 | -->eval_kld 3.83924 | -->eval_penalty 0.04379 | -->eval_penalty_mean 0.09298 | -->eval_penalty_var 0.04919\n",
      "new best model\n",
      "820\t | train_ppx 2095.86035 | train_ppx_doc 2118.36353 | train_loss 1041.86475 | train_loss_rec 1035.56592 | train_kld 4.13396 | train_penalty 0.04330 | train_penalty_mean 0.09250 | train_penalty_var 0.04921\n",
      "840\t | train_ppx 2106.53882 | train_ppx_doc 2120.80029 | train_loss 1042.51526 | train_loss_rec 1036.27808 | train_kld 4.11262 | train_penalty 0.04249 | train_penalty_mean 0.09174 | train_penalty_var 0.04925\n",
      "860\t | train_ppx 2083.01270 | train_ppx_doc 2111.75439 | train_loss 1040.95410 | train_loss_rec 1034.39795 | train_kld 4.46585 | train_penalty 0.04180 | train_penalty_mean 0.09108 | train_penalty_var 0.04927\n",
      "880\t | train_ppx 2071.98901 | train_ppx_doc 2101.97949 | train_loss 1040.19824 | train_loss_rec 1033.65894 | train_kld 4.48356 | train_penalty 0.04111 | train_penalty_mean 0.09041 | train_penalty_var 0.04930\n",
      "900\t | train_ppx 2090.72974 | train_ppx_doc 2102.83350 | train_loss 1041.38220 | train_loss_rec 1034.86035 | train_kld 4.50620 | train_penalty 0.04032 | train_penalty_mean 0.08964 | train_penalty_var 0.04932\n",
      "-->eval_ppx 2167.89282 | -->eval_ppx_doc 2197.15186 | -->eval_loss 1053.85535 | -->eval_loss_rec 1047.76245 | -->eval_kld 4.09970 | -->eval_penalty 0.03986 | -->eval_penalty_mean 0.08920 | -->eval_penalty_var 0.04934\n",
      "new best model\n",
      "920\t | train_ppx 2063.14648 | train_ppx_doc 2086.64917 | train_loss 1039.53955 | train_loss_rec 1033.00134 | train_kld 4.55979 | train_penalty 0.03957 | train_penalty_mean 0.08892 | train_penalty_var 0.04935\n",
      "940\t | train_ppx 2071.20679 | train_ppx_doc 2092.17676 | train_loss 1040.04272 | train_loss_rec 1033.43433 | train_kld 4.65688 | train_penalty 0.03903 | train_penalty_mean 0.08840 | train_penalty_var 0.04937\n",
      "960\t | train_ppx 2058.20996 | train_ppx_doc 2079.04956 | train_loss 1039.15442 | train_loss_rec 1032.64722 | train_kld 4.58809 | train_penalty 0.03838 | train_penalty_mean 0.08777 | train_penalty_var 0.04939\n",
      "980\t | train_ppx 2068.43335 | train_ppx_doc 2085.14136 | train_loss 1039.79114 | train_loss_rec 1033.12842 | train_kld 4.78065 | train_penalty 0.03764 | train_penalty_mean 0.08706 | train_penalty_var 0.04942\n",
      "1000\t | train_ppx 2047.28528 | train_ppx_doc 2073.34399 | train_loss 1038.36926 | train_loss_rec 1031.87134 | train_kld 4.64056 | train_penalty 0.03715 | train_penalty_mean 0.08659 | train_penalty_var 0.04944\n",
      "-->eval_ppx 2157.49683 | -->eval_ppx_doc 2186.86060 | -->eval_loss 1053.05151 | -->eval_loss_rec 1047.08118 | -->eval_kld 4.12274 | -->eval_penalty 0.03695 | -->eval_penalty_mean 0.08642 | -->eval_penalty_var 0.04947\n",
      "new best model\n",
      "1020\t | train_ppx 2038.76123 | train_ppx_doc 2055.45801 | train_loss 1037.78210 | train_loss_rec 1031.17297 | train_kld 4.77160 | train_penalty 0.03675 | train_penalty_mean 0.08623 | train_penalty_var 0.04948\n",
      "1040\t | train_ppx 2042.89417 | train_ppx_doc 2062.07153 | train_loss 1038.02893 | train_loss_rec 1031.56763 | train_kld 4.65238 | train_penalty 0.03618 | train_penalty_mean 0.08568 | train_penalty_var 0.04950\n",
      "1060\t | train_ppx 2035.06543 | train_ppx_doc 2053.92163 | train_loss 1037.47583 | train_loss_rec 1030.66528 | train_kld 5.03285 | train_penalty 0.03556 | train_penalty_mean 0.08509 | train_penalty_var 0.04953\n",
      "1080\t | train_ppx 2027.94067 | train_ppx_doc 2049.69800 | train_loss 1036.97339 | train_loss_rec 1030.27527 | train_kld 4.94592 | train_penalty 0.03504 | train_penalty_mean 0.08460 | train_penalty_var 0.04956\n",
      "1100\t | train_ppx 2033.38647 | train_ppx_doc 2060.31323 | train_loss 1037.31128 | train_loss_rec 1030.51404 | train_kld 5.07178 | train_penalty 0.03451 | train_penalty_mean 0.08408 | train_penalty_var 0.04957\n",
      "-->eval_ppx 2148.75464 | -->eval_ppx_doc 2172.95581 | -->eval_loss 1052.35889 | -->eval_loss_rec 1046.20227 | -->eval_kld 4.44561 | -->eval_penalty 0.03422 | -->eval_penalty_mean 0.08381 | -->eval_penalty_var 0.04959\n",
      "new best model\n",
      "1120\t | train_ppx 2017.69617 | train_ppx_doc 2043.11328 | train_loss 1036.23328 | train_loss_rec 1029.55945 | train_kld 4.97331 | train_penalty 0.03401 | train_penalty_mean 0.08361 | train_penalty_var 0.04960\n",
      "1140\t | train_ppx 2034.63562 | train_ppx_doc 2062.69409 | train_loss 1037.34155 | train_loss_rec 1030.61816 | train_kld 5.05113 | train_penalty 0.03344 | train_penalty_mean 0.08306 | train_penalty_var 0.04961\n",
      "1160\t | train_ppx 2004.96448 | train_ppx_doc 2038.83313 | train_loss 1035.31958 | train_loss_rec 1028.50073 | train_kld 5.17134 | train_penalty 0.03295 | train_penalty_mean 0.08258 | train_penalty_var 0.04963\n",
      "1180\t | train_ppx 2010.71375 | train_ppx_doc 2036.90808 | train_loss 1035.68726 | train_loss_rec 1028.88477 | train_kld 5.17662 | train_penalty 0.03252 | train_penalty_mean 0.08216 | train_penalty_var 0.04964\n",
      "1200\t | train_ppx 2006.56555 | train_ppx_doc 2041.22314 | train_loss 1035.38831 | train_loss_rec 1028.29236 | train_kld 5.48819 | train_penalty 0.03215 | train_penalty_mean 0.08181 | train_penalty_var 0.04965\n",
      "-->eval_ppx 2138.25952 | -->eval_ppx_doc 2160.56372 | -->eval_loss 1051.57214 | -->eval_loss_rec 1045.37427 | -->eval_kld 4.60329 | -->eval_penalty 0.03190 | -->eval_penalty_mean 0.08156 | -->eval_penalty_var 0.04966\n",
      "new best model\n",
      "1220\t | train_ppx 2006.63159 | train_ppx_doc 2033.53772 | train_loss 1035.37195 | train_loss_rec 1028.35889 | train_kld 5.42623 | train_penalty 0.03174 | train_penalty_mean 0.08139 | train_penalty_var 0.04965\n",
      "1240\t | train_ppx 1993.85181 | train_ppx_doc 2020.22522 | train_loss 1034.48877 | train_loss_rec 1027.53748 | train_kld 5.37890 | train_penalty 0.03145 | train_penalty_mean 0.08111 | train_penalty_var 0.04966\n",
      "1260\t | train_ppx 2000.86633 | train_ppx_doc 2030.85730 | train_loss 1034.94922 | train_loss_rec 1027.61267 | train_kld 5.78133 | train_penalty 0.03110 | train_penalty_mean 0.08076 | train_penalty_var 0.04966\n",
      "1280\t | train_ppx 1984.05566 | train_ppx_doc 2016.21899 | train_loss 1033.78235 | train_loss_rec 1026.68335 | train_kld 5.56367 | train_penalty 0.03071 | train_penalty_mean 0.08036 | train_penalty_var 0.04965\n",
      "1300\t | train_ppx 1989.49268 | train_ppx_doc 2007.42786 | train_loss 1034.13965 | train_loss_rec 1027.16138 | train_kld 5.45771 | train_penalty 0.03041 | train_penalty_mean 0.08005 | train_penalty_var 0.04964\n",
      "-->eval_ppx 2131.64844 | -->eval_ppx_doc 2149.54785 | -->eval_loss 1051.06543 | -->eval_loss_rec 1044.69397 | -->eval_kld 4.85923 | -->eval_penalty 0.03025 | -->eval_penalty_mean 0.07988 | -->eval_penalty_var 0.04963\n",
      "new best model\n",
      "1320\t | train_ppx 2001.97437 | train_ppx_doc 2017.54712 | train_loss 1034.97388 | train_loss_rec 1027.83240 | train_kld 5.63672 | train_penalty 0.03009 | train_penalty_mean 0.07971 | train_penalty_var 0.04961\n",
      "1340\t | train_ppx 1969.67383 | train_ppx_doc 1997.98987 | train_loss 1032.75513 | train_loss_rec 1025.62317 | train_kld 5.63480 | train_penalty 0.02995 | train_penalty_mean 0.07954 | train_penalty_var 0.04959\n",
      "1360\t | train_ppx 1972.63367 | train_ppx_doc 1993.06470 | train_loss 1032.93860 | train_loss_rec 1025.88477 | train_kld 5.57732 | train_penalty 0.02953 | train_penalty_mean 0.07912 | train_penalty_var 0.04959\n",
      "1380\t | train_ppx 1963.29199 | train_ppx_doc 1990.14929 | train_loss 1032.27429 | train_loss_rec 1025.13672 | train_kld 5.68005 | train_penalty 0.02915 | train_penalty_mean 0.07873 | train_penalty_var 0.04958\n",
      "1400\t | train_ppx 1977.46777 | train_ppx_doc 2005.08020 | train_loss 1033.23901 | train_loss_rec 1026.13367 | train_kld 5.66118 | train_penalty 0.02888 | train_penalty_mean 0.07844 | train_penalty_var 0.04956\n",
      "-->eval_ppx 2121.60229 | -->eval_ppx_doc 2144.58643 | -->eval_loss 1050.34521 | -->eval_loss_rec 1044.11279 | -->eval_kld 4.79355 | -->eval_penalty 0.02877 | -->eval_penalty_mean 0.07832 | -->eval_penalty_var 0.04955\n",
      "new best model\n",
      "1420\t | train_ppx 1969.37244 | train_ppx_doc 1989.46130 | train_loss 1032.66675 | train_loss_rec 1025.54456 | train_kld 5.69256 | train_penalty 0.02859 | train_penalty_mean 0.07816 | train_penalty_var 0.04957\n",
      "1440\t | train_ppx 1969.56494 | train_ppx_doc 1995.82080 | train_loss 1032.67004 | train_loss_rec 1025.40649 | train_kld 5.84395 | train_penalty 0.02839 | train_penalty_mean 0.07795 | train_penalty_var 0.04956\n",
      "1460\t | train_ppx 1952.56104 | train_ppx_doc 1981.71655 | train_loss 1031.47766 | train_loss_rec 1024.18762 | train_kld 5.88405 | train_penalty 0.02812 | train_penalty_mean 0.07768 | train_penalty_var 0.04956\n",
      "1480\t | train_ppx 1954.32996 | train_ppx_doc 1979.33191 | train_loss 1031.59045 | train_loss_rec 1024.22595 | train_kld 5.96872 | train_penalty 0.02791 | train_penalty_mean 0.07747 | train_penalty_var 0.04956\n",
      "1500\t | train_ppx 1957.54297 | train_ppx_doc 1982.56995 | train_loss 1031.80017 | train_loss_rec 1024.52100 | train_kld 5.89703 | train_penalty 0.02764 | train_penalty_mean 0.07720 | train_penalty_var 0.04956\n",
      "-->eval_ppx 2109.50317 | -->eval_ppx_doc 2126.66064 | -->eval_loss 1049.49500 | -->eval_loss_rec 1042.92566 | -->eval_kld 5.19783 | -->eval_penalty 0.02744 | -->eval_penalty_mean 0.07700 | -->eval_penalty_var 0.04956\n",
      "new best model\n",
      "1520\t | train_ppx 1954.21997 | train_ppx_doc 1974.70789 | train_loss 1031.55078 | train_loss_rec 1024.26172 | train_kld 5.92547 | train_penalty 0.02727 | train_penalty_mean 0.07683 | train_penalty_var 0.04955\n",
      "1540\t | train_ppx 1953.39258 | train_ppx_doc 1978.70728 | train_loss 1031.47974 | train_loss_rec 1024.00916 | train_kld 6.12026 | train_penalty 0.02701 | train_penalty_mean 0.07655 | train_penalty_var 0.04954\n",
      "1560\t | train_ppx 1944.19763 | train_ppx_doc 1963.32288 | train_loss 1030.82922 | train_loss_rec 1023.47125 | train_kld 6.01676 | train_penalty 0.02683 | train_penalty_mean 0.07636 | train_penalty_var 0.04953\n",
      "1580\t | train_ppx 1941.52393 | train_ppx_doc 1966.71008 | train_loss 1030.63245 | train_loss_rec 1023.13513 | train_kld 6.16578 | train_penalty 0.02663 | train_penalty_mean 0.07615 | train_penalty_var 0.04953\n",
      "1600\t | train_ppx 1928.57568 | train_ppx_doc 1961.10913 | train_loss 1029.71436 | train_loss_rec 1022.16107 | train_kld 6.23010 | train_penalty 0.02646 | train_penalty_mean 0.07598 | train_penalty_var 0.04952\n",
      "-->eval_ppx 2107.48218 | -->eval_ppx_doc 2125.19678 | -->eval_loss 1049.31055 | -->eval_loss_rec 1042.61121 | -->eval_kld 5.38084 | -->eval_penalty 0.02637 | -->eval_penalty_mean 0.07588 | -->eval_penalty_var 0.04951\n",
      "new best model\n",
      "1620\t | train_ppx 1929.71082 | train_ppx_doc 1958.64941 | train_loss 1029.78137 | train_loss_rec 1022.24902 | train_kld 6.22239 | train_penalty 0.02620 | train_penalty_mean 0.07571 | train_penalty_var 0.04951\n",
      "1640\t | train_ppx 1916.09570 | train_ppx_doc 1951.22083 | train_loss 1028.80066 | train_loss_rec 1021.16302 | train_kld 6.34556 | train_penalty 0.02584 | train_penalty_mean 0.07534 | train_penalty_var 0.04950\n",
      "1660\t | train_ppx 1913.40955 | train_ppx_doc 1941.05554 | train_loss 1028.59265 | train_loss_rec 1021.09576 | train_kld 6.22215 | train_penalty 0.02550 | train_penalty_mean 0.07499 | train_penalty_var 0.04950\n",
      "1680\t | train_ppx 1925.93640 | train_ppx_doc 1948.15649 | train_loss 1029.46606 | train_loss_rec 1021.90723 | train_kld 6.29785 | train_penalty 0.02522 | train_penalty_mean 0.07471 | train_penalty_var 0.04949\n",
      "1700\t | train_ppx 1917.51147 | train_ppx_doc 1939.40698 | train_loss 1028.85950 | train_loss_rec 1021.29718 | train_kld 6.31173 | train_penalty 0.02501 | train_penalty_mean 0.07450 | train_penalty_var 0.04949\n",
      "-->eval_ppx 2098.60132 | -->eval_ppx_doc 2114.20288 | -->eval_loss 1048.66028 | -->eval_loss_rec 1041.88354 | -->eval_kld 5.53038 | -->eval_penalty 0.02493 | -->eval_penalty_mean 0.07441 | -->eval_penalty_var 0.04949\n",
      "new best model\n",
      "1720\t | train_ppx 1936.78967 | train_ppx_doc 1951.52881 | train_loss 1030.21387 | train_loss_rec 1022.48340 | train_kld 6.48565 | train_penalty 0.02490 | train_penalty_mean 0.07438 | train_penalty_var 0.04949\n",
      "1740\t | train_ppx 1911.75708 | train_ppx_doc 1932.60413 | train_loss 1028.43298 | train_loss_rec 1020.77258 | train_kld 6.42803 | train_penalty 0.02465 | train_penalty_mean 0.07414 | train_penalty_var 0.04949\n",
      "1760\t | train_ppx 1931.36047 | train_ppx_doc 1953.76709 | train_loss 1029.81177 | train_loss_rec 1022.01324 | train_kld 6.57417 | train_penalty 0.02449 | train_penalty_mean 0.07397 | train_penalty_var 0.04948\n",
      "1780\t | train_ppx 1907.23450 | train_ppx_doc 1933.05396 | train_loss 1028.09631 | train_loss_rec 1020.62134 | train_kld 6.25712 | train_penalty 0.02436 | train_penalty_mean 0.07383 | train_penalty_var 0.04947\n",
      "1800\t | train_ppx 1912.16370 | train_ppx_doc 1935.29138 | train_loss 1028.43237 | train_loss_rec 1020.68414 | train_kld 6.54521 | train_penalty 0.02406 | train_penalty_mean 0.07354 | train_penalty_var 0.04948\n",
      "-->eval_ppx 2094.05420 | -->eval_ppx_doc 2109.68408 | -->eval_loss 1048.31201 | -->eval_loss_rec 1041.56226 | -->eval_kld 5.55470 | -->eval_penalty 0.02390 | -->eval_penalty_mean 0.07337 | -->eval_penalty_var 0.04947\n",
      "new best model\n",
      "1820\t | train_ppx 1906.45251 | train_ppx_doc 1936.74353 | train_loss 1028.01147 | train_loss_rec 1020.40424 | train_kld 6.41856 | train_penalty 0.02377 | train_penalty_mean 0.07325 | train_penalty_var 0.04947\n",
      "1840\t | train_ppx 1910.06409 | train_ppx_doc 1930.79321 | train_loss 1028.25562 | train_loss_rec 1020.41980 | train_kld 6.66012 | train_penalty 0.02351 | train_penalty_mean 0.07297 | train_penalty_var 0.04946\n",
      "1860\t | train_ppx 1917.76477 | train_ppx_doc 1936.82764 | train_loss 1028.78430 | train_loss_rec 1021.16748 | train_kld 6.45957 | train_penalty 0.02315 | train_penalty_mean 0.07261 | train_penalty_var 0.04947\n",
      "1880\t | train_ppx 1889.92236 | train_ppx_doc 1915.00330 | train_loss 1026.78687 | train_loss_rec 1018.87463 | train_kld 6.76424 | train_penalty 0.02296 | train_penalty_mean 0.07242 | train_penalty_var 0.04946\n",
      "1900\t | train_ppx 1892.90942 | train_ppx_doc 1914.35693 | train_loss 1026.99011 | train_loss_rec 1019.16333 | train_kld 6.69021 | train_penalty 0.02273 | train_penalty_mean 0.07220 | train_penalty_var 0.04947\n",
      "-->eval_ppx 2088.12231 | -->eval_ppx_doc 2099.39502 | -->eval_loss 1047.85999 | -->eval_loss_rec 1040.83240 | -->eval_kld 5.89590 | -->eval_penalty 0.02263 | -->eval_penalty_mean 0.07209 | -->eval_penalty_var 0.04946\n",
      "new best model\n",
      "1920\t | train_ppx 1892.97351 | train_ppx_doc 1920.99194 | train_loss 1026.98425 | train_loss_rec 1019.09589 | train_kld 6.76234 | train_penalty 0.02252 | train_penalty_mean 0.07197 | train_penalty_var 0.04945\n",
      "1940\t | train_ppx 1886.76270 | train_ppx_doc 1911.81909 | train_loss 1026.52649 | train_loss_rec 1018.81152 | train_kld 6.60000 | train_penalty 0.02230 | train_penalty_mean 0.07175 | train_penalty_var 0.04945\n",
      "1960\t | train_ppx 1876.13342 | train_ppx_doc 1904.61987 | train_loss 1025.75220 | train_loss_rec 1017.70367 | train_kld 6.93947 | train_penalty 0.02218 | train_penalty_mean 0.07161 | train_penalty_var 0.04943\n",
      "1980\t | train_ppx 1876.76685 | train_ppx_doc 1901.07031 | train_loss 1025.79565 | train_loss_rec 1017.95825 | train_kld 6.73093 | train_penalty 0.02213 | train_penalty_mean 0.07155 | train_penalty_var 0.04941\n",
      "2000\t | train_ppx 1880.81262 | train_ppx_doc 1912.21924 | train_loss 1026.07495 | train_loss_rec 1018.25848 | train_kld 6.72359 | train_penalty 0.02186 | train_penalty_mean 0.07127 | train_penalty_var 0.04941\n",
      "-->eval_ppx 2086.01660 | -->eval_ppx_doc 2098.42627 | -->eval_loss 1047.67725 | -->eval_loss_rec 1040.95520 | -->eval_kld 5.63500 | -->eval_penalty 0.02174 | -->eval_penalty_mean 0.07116 | -->eval_penalty_var 0.04942\n",
      "new best model\n"
     ]
    }
   ],
   "source": [
    "from src.task import TopicModelTask\n",
    "from src.components import Conf, GSM\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from src.dataset import load_news_data, load_kos_data\n",
    "from collections import namedtuple\n",
    "from os import path\n",
    "\n",
    "ARGS = namedtuple(\"ARGS\", [\"lr\", \"epochs\", \"evaluate_every\", \"batch_size\", \"data_dir\", \"runs_dir\", \"export_dir\", \"device\"])\n",
    "args = ARGS(\n",
    "    lr=0.001,\n",
    "    epochs=100,\n",
    "    evaluate_every=5,\n",
    "    batch_size=100,\n",
    "    data_dir=\"data\",\n",
    "    runs_dir=\"runs\",\n",
    "    export_dir=\"export_dir\",\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "def write_summary(writer, stat, step, prefix='train'):\n",
    "    stat = stat.get_dict()\n",
    "    for k, v in stat.items():\n",
    "        writer.add_scalars(k, {prefix: v}, global_step=step)\n",
    "\n",
    "\n",
    "def save_checkpoint(args, model):\n",
    "    filename = path.join(args.export_dir, 'model_best.pt')\n",
    "    state_dict = model.to(\"cpu\").state_dict()\n",
    "    torch.save(state_dict, filename)\n",
    "    model.to(torch.device(args.device))\n",
    "\n",
    "\n",
    "def save_topics(args, vocab, topic_prob, epoch, topk=100):\n",
    "    topic_prob = topic_prob.detach()\n",
    "    values, indices = torch.topk(topic_prob, k=topk, dim=-1)\n",
    "\n",
    "    topics = []\n",
    "    for t in indices:\n",
    "        topics.append(' '.join([vocab.get_word(i.item()) for i in t]))\n",
    "\n",
    "\n",
    "    with open(path.join(args.export_dir, \"topic-{}.topics\".format(epoch)), 'w') as f:\n",
    "        f.write('\\n'.join(topics))\n",
    "\n",
    "    str_values = []\n",
    "    for t in values:\n",
    "        str_values.append(' '.join([str(v) for v in t]))\n",
    "\n",
    "    with open(path.join(args.export_dir, \"topic-{}.values\".format(epoch)), 'w') as f:\n",
    "        f.write('\\n'.join(str_values))\n",
    "\n",
    "\n",
    "device = torch.device(args.device)\n",
    "\n",
    "conf = Conf(vocab_size=6906, hidden_size=65, latent_size=20)\n",
    "model = GSM(conf).to(device)\n",
    "\n",
    "task = TopicModelTask(\"topic model\", args)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "# load datasets\n",
    "train_loader, dev_loader, test_loader, vocab = load_kos_data(args, device)\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "writer = SummaryWriter(path.join(args.runs_dir, \"gsm-3\"))\n",
    "\n",
    "for e in range(1, args.epochs + 1):\n",
    "    stats = task.train(model, optim,  train_loader, device)\n",
    "\n",
    "    print(f\"{task.global_step}\\t | \" + stats.description('train_'))\n",
    "    write_summary(writer, stats, task.global_step, 'train')\n",
    "\n",
    "    if e % args.evaluate_every == 0:\n",
    "        stats = task.eval(model, dev_loader, device)\n",
    "        print(stats.description('-->eval_'))\n",
    "        write_summary(writer, stats, task.global_step, 'eval')\n",
    "\n",
    "        if stats.get_value(\"loss\") < best_loss:\n",
    "            print(\"new best model\")\n",
    "            best_loss = stats.get_value(\"loss\")\n",
    "            save_checkpoint(args, model)\n",
    "            topics = model.get_topics()\n",
    "            save_topics(args, vocab, topics, e, topk=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic\t0\n",
      "iraq iraqi baghdad fallujah mosque saddam war sadr marines military\n",
      "topic\t1\n",
      "deficit gdp scalia tax billion bush budget income rationing jobs\n",
      "topic\t2\n",
      "delay ethics bell party rosenberg committee dlc complaint texas bunning\n",
      "topic\t3\n",
      "november dean vaantirepublican hoodies chedrcheez sappy wclathe verification exit kingelection\n",
      "topic\t4\n",
      "kerry dean percent poll edwards bush results clark voters primary\n",
      "topic\t5\n",
      "november petraeus electoral iraq charge fallujah account verified house sunzoo\n",
      "topic\t6\n",
      "bush iraq war administration president bushs kerry john people media\n",
      "topic\t7\n",
      "november account poll electoral republicans exit return governor allegory kingelection\n",
      "topic\t8\n",
      "kerry convention bush bloggers campaign news media john black kerrys\n",
      "topic\t9\n",
      "november voting account electoral house governor polls republicans senate election\n",
      "topic\t10\n",
      "lieb dean clark kerry edwards sharpton lieberman pdf arg kucinich\n",
      "topic\t11\n",
      "bush tax marriage gay health bill care president billion administration\n",
      "topic\t12\n",
      "race seat district senate republican campaign candidate party house state\n",
      "topic\t13\n",
      "district seat race senate romero house million specter afscme wilson\n",
      "topic\t14\n",
      "kerry bloggers campaign convention bush party media general debate news\n",
      "topic\t15\n",
      "november senate house republicans election races voting governor democrats electoral\n",
      "topic\t16\n",
      "bush kerry media john president news bloggers people convention time\n",
      "topic\t17\n",
      "administration bush terrorism iraq intelligence counterterrorism terrorists war terrorist fbi\n",
      "topic\t18\n",
      "iraq war iraqi american officials military abu interrogation ghraib soldiers\n",
      "topic\t19\n",
      "bush film kerry iraq media fox john news war time\n"
     ]
    }
   ],
   "source": [
    "def print_topics(vocab, topics, topk=10):\n",
    "    topics = topics.detach()\n",
    "    values, indices = torch.topk(topics, k=topk, dim=-1)\n",
    "\n",
    "    for idx, t in enumerate(indices):\n",
    "        print(f\"topic\\t{idx}\")\n",
    "        print(' '.join([vocab.get_word(i.item()) for i in t]))\n",
    "\n",
    "print_topics(vocab, topics)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see the 20 topic extracted are mostly related to politics, wars and election.\n",
    "Yet, the final perplexity obtained is much better w.r.t. the Dirichlet Mixture model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}